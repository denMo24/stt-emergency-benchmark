{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48994ed9-a545-4cfc-b17e-7b8397f7e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────── Part 1: Parameter ───────────────\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Number of audiofiles you want to process\n",
    "# None = all files\n",
    "num_files = 2000  \n",
    "\n",
    "# Adjust this directory according where you saved your audio files!\n",
    "notebook_dir = Path.home() / \"stanic_audio\"\n",
    "\n",
    "# Output-Directory/File for this session\n",
    "timestamp = datetime.now().strftime(\"%d%m%y_%H%M\")\n",
    "output_dir = notebook_dir / f\"results_{timestamp}\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Verarbeite bis zu {num_files if num_files is not None else 'alle'} Dateien aus {notebook_dir}\")\n",
    "print(f\"Ergebnisse landen in {output_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9f587-0c34-4825-9b4a-1394d744ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────── Part 2: Loop & Transcription ───────────────\n",
    "import json\n",
    "import torch\n",
    "from speechbrain.inference.ASR import WhisperASR\n",
    "\n",
    "# 1. Choosing device (CPU/GPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Verwende Device: {device}\")\n",
    "\n",
    "# 2. Load model\n",
    "asr_model = WhisperASR.from_hparams(\n",
    "    source=\"speechbrain/whisper_rescuespeech\",\n",
    "    savedir=\"pretrained_models/whisper_rescuespeech\",\n",
    "    overrides={\n",
    "        \"decoder.bos_index\": None,\n",
    "        \"decoder.eos_index\": None,\n",
    "    },\n",
    "    overrides_must_match=False,\n",
    "    run_opts={\"device\": device},\n",
    ")\n",
    "\n",
    "# 3. List all .wav files recursively in the notebook directory\n",
    "wav_files = sorted(notebook_dir.rglob(\"*.wav\"))\n",
    "if num_files is not None:\n",
    "    wav_files = wav_files[:num_files]\n",
    "\n",
    "print(f\"Found WAVs: {len(wav_files)}\")\n",
    "\n",
    "# 4. Iterate through all files and transcribe\n",
    "for idx, wav_path in enumerate(wav_files, 1):\n",
    "    print(f\"[{idx}/{len(wav_files)}] Transcribe: {wav_path.name}\")\n",
    "    \n",
    "    # Transcription\n",
    "    segments = asr_model.transcribe_file(str(wav_path))\n",
    "    \n",
    "    # JSON structure in the desired format\n",
    "    json_output = [\n",
    "        {\n",
    "            \"start\": seg.start,\n",
    "            \"end\": seg.end,\n",
    "            \"text\": seg.words,\n",
    "            \"words\": seg.words.split(),\n",
    "        }\n",
    "        for seg in segments\n",
    "    ]\n",
    "    \n",
    "    # Filename and timestamp\n",
    "    basename  = wav_path.stem\n",
    "    model_tag = \"speechbrain_whisper_rescuespeech\"\n",
    "    stamp     = datetime.now().strftime(\"%d%m%y_%H%M%S\")\n",
    "    out_name  = f\"{basename}_{model_tag}_{stamp}.json\"\n",
    "    \n",
    "    # Save in output_dir\n",
    "    out_path = output_dir / out_name\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_output, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\" → Saved: {out_path.name}\")\n",
    "\n",
    "print(\"✅ Done!\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fd485a-ca63-4c00-9e85-6b76b8581eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SpeechBrain Env",
   "language": "python",
   "name": "speechbrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
