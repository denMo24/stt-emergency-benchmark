{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "565dce33-a6bf-4bcf-9402-b97d7bb26e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         id       WER    S   D   I  S_med_diagnosis  \\\n",
      "0  6765e8a4a84bdac1bcd5a73e  0.363198  445  74  66               12   \n",
      "\n",
      "   D_med_diagnosis  I_med_diagnosis     m-WER  \n",
      "0                6                3  0.074733  \n"
     ]
    }
   ],
   "source": [
    "# Ziel: Für eine wählbare Anzahl von Transkripten (1 oder \"all\") Standard-WER und m-WER (S/D/I_med_diagnosis) berechnen.\n",
    "\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from jiwer import process_words, collect_error_counts\n",
    "\n",
    "# -- Einstellungen --\n",
    "NUM_TRANSCRIPTS = 1   # Setze auf eine Zahl oder \"all\" für alle Datensätze\n",
    "\n",
    "# MongoDB-Verbindung\n",
    "client = MongoClient(\"mongodb://localhost:27018/\")\n",
    "col = client[\"transcriptions\"][\"transcripts_denis\"]\n",
    "\n",
    "# Medizinische Wortlisten laden\n",
    "cleaned = pd.read_csv(\"cleaned_ger_synonyms.csv\")  # aus vorherigem Notebook\n",
    "# Sets für Nomen und Adjektive\n",
    "noun_set = set(w for phrase in cleaned[\"only_nouns\"] for w in str(phrase).split() if w)\n",
    "adj_set  = set(w for phrase in cleaned[\"adjectives\"] for w in str(phrase).split() if w)\n",
    "\n",
    "# Funktion zur einfachen Normalisierung\n",
    "def normalize_simple(text: str) -> str:\n",
    "    t = text.lower()\n",
    "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "# Query vorbereiten\n",
    "cursor = col.find({}, {\"_id\":1, \"src_wer_denis\":1, \"text\":1})\n",
    "if NUM_TRANSCRIPTS != \"all\":\n",
    "    cursor = cursor.limit(int(NUM_TRANSCRIPTS))\n",
    "\n",
    "# Ergebnisse sammeln\n",
    "results = []\n",
    "for doc in cursor:\n",
    "    ref = doc.get(\"src_wer_denis\", \"\")\n",
    "    hyp = normalize_simple(doc.get(\"text\", \"\"))  # Hypothese nachsimple normalization\n",
    "    \n",
    "    # WER-Alignment\n",
    "    out = process_words(ref, hyp)\n",
    "    subs, ins, dels = collect_error_counts(out)\n",
    "\n",
    "    # Standard-WER-Stats\n",
    "    S, D, I = sum(subs.values()), sum(dels.values()), sum(ins.values())\n",
    "    N = len(ref.split())\n",
    "    wer_score = out.wer\n",
    "\n",
    "    # Medizinische Fehler klassifizieren\n",
    "    S_med = sum(cnt for (r,h), cnt in subs.items() if r in noun_set or r in adj_set)\n",
    "    D_med = sum(cnt for r, cnt in dels.items()        if r in noun_set or r in adj_set)\n",
    "    I_med = sum(cnt for h, cnt in ins.items()         if h in noun_set or h in adj_set)\n",
    "\n",
    "    # m-WER\n",
    "    total_med_ref = sum(1 for w in ref.split() if w in noun_set or w in adj_set)\n",
    "    m_wer = (S_med + D_med + I_med) / total_med_ref if total_med_ref else None\n",
    "\n",
    "    results.append({\n",
    "        \"id\": str(doc[\"_id\"]),\n",
    "        \"WER\": wer_score,\n",
    "        \"S\": S, \"D\": D, \"I\": I,\n",
    "        \"S_med_diagnosis\": S_med,\n",
    "        \"D_med_diagnosis\": D_med,\n",
    "        \"I_med_diagnosis\": I_med,\n",
    "        \"m-WER\": m_wer\n",
    "    })\n",
    "\n",
    "# Ausgabe als DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f935a6d1-7f34-4fb9-8109-a75bd64dd9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Transcripts verarbeitet und in 'wer_mwer_results.json' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# 0. (Einmalig) Dependencies installieren, falls nötig:\n",
    "# !pip install jiwer pymongo pandas\n",
    "\n",
    "import re\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from jiwer import process_words, collect_error_counts\n",
    "\n",
    "# -- Einstellungen --\n",
    "NUM_TRANSCRIPTS = 1   # Zahl oder \"all\"\n",
    "OUTPUT_FILE     = \"wer_mwer_results.json\"\n",
    "\n",
    "# 1. MongoDB-Verbindung\n",
    "client = MongoClient(\"mongodb://localhost:27018/\")\n",
    "col = client[\"transcriptions\"][\"transcripts_denis\"]\n",
    "\n",
    "# 2. Medizinische Wortlisten laden\n",
    "cleaned = pd.read_csv(\"cleaned_ger_synonyms.csv\")\n",
    "noun_set = set(w for phrase in cleaned[\"only_nouns\"] for w in str(phrase).split() if w)\n",
    "adj_set  = set(w for phrase in cleaned[\"adjectives\"] for w in str(phrase).split() if w)\n",
    "\n",
    "# 3. Simple Normalisierung\n",
    "def normalize_simple(text: str) -> str:\n",
    "    t = text.lower()\n",
    "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "# 4. Cursor vorbereiten\n",
    "cursor = col.find({}, {\"_id\":1, \"src_wer_denis\":1, \"text\":1})\n",
    "if NUM_TRANSCRIPTS != \"all\":\n",
    "    cursor = cursor.limit(int(NUM_TRANSCRIPTS))\n",
    "\n",
    "# 5. Ergebnisse erzeugen\n",
    "results = []\n",
    "for doc in cursor:\n",
    "    tid = str(doc[\"_id\"])\n",
    "    ref = normalize_simple(doc.get(\"src_wer_denis\", \"\"))\n",
    "    hyp = normalize_simple(doc.get(\"text\", \"\"))\n",
    "\n",
    "    out       = process_words(ref, hyp)\n",
    "    subs, ins, dels = collect_error_counts(out)\n",
    "\n",
    "    # Standard-WER-Zahlen\n",
    "    S, D, I = sum(subs.values()), sum(dels.values()), sum(ins.values())\n",
    "\n",
    "    # Token-Listen\n",
    "    subs_tokens = [r for (r,h), cnt in subs.items() for _ in range(cnt)]\n",
    "    del_tokens  = [r for r, cnt in dels.items()   for _ in range(cnt)]\n",
    "    ins_tokens  = [h for h, cnt in ins.items()    for _ in range(cnt)]\n",
    "\n",
    "    # med-Filter\n",
    "    med_subs = [t for t in subs_tokens if t in noun_set or t in adj_set]\n",
    "    med_dels = [t for t in del_tokens  if t in noun_set or t in adj_set]\n",
    "    med_ins  = [t for t in ins_tokens  if t in noun_set or t in adj_set]\n",
    "\n",
    "    # m-WER\n",
    "    total_med_ref = sum(1 for w in ref.split() if w in noun_set or w in adj_set)\n",
    "    m_wer = (len(med_subs) + len(med_dels) + len(med_ins)) / total_med_ref if total_med_ref else None\n",
    "\n",
    "    results.append({\n",
    "        \"id\":                 tid,\n",
    "        \"WER\":                out.wer,\n",
    "        \"S\":                  S,\n",
    "        \"D\":                  D,\n",
    "        \"I\":                  I,\n",
    "        \"S_med_diagnosis\":    med_subs,\n",
    "        \"D_med_diagnosis\":    med_dels,\n",
    "        \"I_med_diagnosis\":    med_ins,\n",
    "        \"m-WER\":              m_wer\n",
    "    })\n",
    "\n",
    "# 6. Lokal als JSON speichern\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"{len(results)} Transcripts verarbeitet und in '{OUTPUT_FILE}' gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e6084-2272-4a5c-bb39-fb70f606a1bb",
   "metadata": {},
   "source": [
    "# Step by Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebf907f3-5164-4ab0-81d2-0164819c1565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Transcripts verarbeitet. Ergebnisse gespeichert in 'wer_token_sources.json'.\n"
     ]
    }
   ],
   "source": [
    "# Ziel: Für jedes Transcript die WER-Operationen extrahieren, dabei klar benennen, ob Tokens aus Ref- oder Hyp-Text stammen,\n",
    "# und das Ergebnis als JSON-Datei in deinem Notebook-Verzeichnis speichern.\n",
    "\n",
    "import re\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from jiwer import process_words, collect_error_counts\n",
    "\n",
    "# 0. (Einmalig) Dependencies installieren, falls nötig:\n",
    "# !pip install jiwer pymongo pandas\n",
    "\n",
    "# -- Einstellungen --\n",
    "NUM_TRANSCRIPTS = 1   # Zahl oder \"all\"\n",
    "OUTPUT_FILE     = \"wer_token_sources.json\"\n",
    "\n",
    "# 1. MongoDB-Verbindung\n",
    "client = MongoClient(\"mongodb://localhost:27018/\")\n",
    "col = client[\"transcriptions\"][\"transcripts_denis\"]\n",
    "\n",
    "# 2. Simple Normalisierung (da deine src_wer_denis/text_wer_denis bereits bereinigt sind):\n",
    "def normalize_simple(text: str) -> str:\n",
    "    t = text.lower()\n",
    "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "# 3. Cursor vorbereiten\n",
    "cursor = col.find({}, {\"_id\":1, \"src_wer_denis\":1, \"text_wer_denis\":1})\n",
    "if NUM_TRANSCRIPTS != \"all\":\n",
    "    cursor = cursor.limit(int(NUM_TRANSCRIPTS))\n",
    "\n",
    "results = []\n",
    "for doc in cursor:\n",
    "    tid = str(doc[\"_id\"])\n",
    "    ref = normalize_simple(doc.get(\"src_wer_denis\", \"\"))\n",
    "    hyp = normalize_simple(doc.get(\"text_wer_denis\", \"\"))\n",
    "\n",
    "    # 4. WER-Alignment und Fehlerzählungen\n",
    "    out       = process_words(ref, hyp)\n",
    "    subs_dict, ins_dict, del_dict = collect_error_counts(out)\n",
    "\n",
    "    # 5. Token-Listen mit Quellangabe\n",
    "    subs_ref_tokens = [r for (r, h), cnt in subs_dict.items() for _ in range(cnt)]\n",
    "    del_ref_tokens  = [r for r, cnt in del_dict.items()   for _ in range(cnt)]\n",
    "    ins_hyp_tokens  = [h for h, cnt in ins_dict.items()    for _ in range(cnt)]\n",
    "\n",
    "    # 6. Ergebnis-Dict je Transcript\n",
    "    results.append({\n",
    "        \"id\": tid,\n",
    "        \"wer\": out.wer,\n",
    "        \"subs_ref_tokens\": subs_ref_tokens,\n",
    "        \"del_ref_tokens\":  del_ref_tokens,\n",
    "        \"ins_hyp_tokens\":  ins_hyp_tokens\n",
    "    })\n",
    "\n",
    "# 7. Lokal als JSON speichern\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"{len(results)} Transcripts verarbeitet. Ergebnisse gespeichert in '{OUTPUT_FILE}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41f36e5f-9888-4efb-9624-88de1a08b5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substitutions total: 325, medical: 36\n",
      "Deletions     total: 102, medical: 7\n",
      "Insertions    total: 64, medical: 2\n"
     ]
    }
   ],
   "source": [
    "# Ziel: Erweiterte Klassifikation für Substitutions-, Deletions- und Insertions-Tokens\n",
    "# wobei nun zusätzlich mittels TF-IDF und Cosine-Similarity (Cutoff=0.70) unscharfe Matches erlaubt sind.\n",
    "# Ergebnisse werden in CSVs mit Prefix \"result_\" gespeichert.\n",
    "\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. WER-Token-JSON laden\n",
    "WER_JSON = \"wer_token_sources.json\"\n",
    "wer_data = json.load(open(WER_JSON, encoding=\"utf-8\"))\n",
    "\n",
    "# 2. Medizinisches Vokabular aus bisherigen Lexika laden\n",
    "cleaned = pd.read_csv(\"lexikon_cleaned_ger_synonyms.csv\")\n",
    "noun_set = set(w for phrase in cleaned[\"only_nouns\"].dropna() for w in phrase.split())\n",
    "adj_set  = set(w for phrase in cleaned[\"adjectives\"].dropna()  for w in phrase.split())\n",
    "\n",
    "atc = pd.read_csv(\"lexikon_ATC-Bedeutung_final_noarticles.csv\")\n",
    "atc_terms = atc[\"ATC-Bedeutung_cleaned\"].dropna().tolist()\n",
    "\n",
    "ling = pd.read_csv(\"lexikon_deDE15LinguisticVariant_final_noarticles.csv\")\n",
    "comp_terms = ling[\"COMPONENT_cleaned\"].dropna().tolist()\n",
    "\n",
    "def tokenize_list(phrases):\n",
    "    toks = set()\n",
    "    for p in phrases:\n",
    "        for w in re.sub(r\"[^\\w\\säöüß]\", \" \", p.lower()).split():\n",
    "            if w and w.isalpha():\n",
    "                toks.add(w)\n",
    "    return toks\n",
    "\n",
    "atc_set  = tokenize_list(atc_terms)\n",
    "comp_set = tokenize_list(comp_terms)\n",
    "\n",
    "med_vocab = sorted(noun_set.union(adj_set, atc_set, comp_set))\n",
    "\n",
    "# 3. TF-IDF Vectorizer vorbereiten (char-level ngrams für Fuzzy-Matching)\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2,4))\n",
    "X_vocab = vectorizer.fit_transform(med_vocab)\n",
    "\n",
    "# 4. Hilfsfunktion für medizinisches Matching\n",
    "def is_medical(token, cutoff=0.70):\n",
    "    # Exaktes Matching\n",
    "    if token in med_vocab:\n",
    "        return True\n",
    "    # Fuzzy Matching via Cosine-Similarity\n",
    "    vec = vectorizer.transform([token])\n",
    "    sims = cosine_similarity(vec, X_vocab).flatten()\n",
    "    return sims.max() >= cutoff\n",
    "\n",
    "# 5. Token-Listen aus WER-Daten aggregieren\n",
    "subs = []\n",
    "dels = []\n",
    "ins  = []\n",
    "for entry in wer_data:\n",
    "    subs += entry.get(\"subs_ref_tokens\", [])\n",
    "    dels += entry.get(\"del_ref_tokens\", [])\n",
    "    ins  += entry.get(\"ins_hyp_tokens\", [])\n",
    "\n",
    "# 6. DataFrames erstellen und Klassifikation durchführen\n",
    "df_subs = pd.DataFrame({\"token\": subs})\n",
    "df_subs[\"is_medical\"] = df_subs[\"token\"].apply(is_medical)\n",
    "\n",
    "df_dels = pd.DataFrame({\"token\": dels})\n",
    "df_dels[\"is_medical\"] = df_dels[\"token\"].apply(is_medical)\n",
    "\n",
    "df_ins = pd.DataFrame({\"token\": ins})\n",
    "df_ins[\"is_medical\"] = df_ins[\"token\"].apply(is_medical)\n",
    "\n",
    "# 7. Ergebnisse speichern\n",
    "df_subs.to_csv(\"result_subs_medical.csv\", index=False)\n",
    "df_dels.to_csv(\"result_dels_medical.csv\", index=False)\n",
    "df_ins.to_csv(\"result_ins_medical.csv\", index=False)\n",
    "\n",
    "# 8. Zusammenfassung ausgeben\n",
    "print(f\"Substitutions total: {len(df_subs)}, medical: {df_subs['is_medical'].sum()}\")\n",
    "print(f\"Deletions     total: {len(df_dels)}, medical: {df_dels['is_medical'].sum()}\")\n",
    "print(f\"Insertions    total: {len(df_ins)}, medical: {df_ins['is_medical'].sum()}\")\n",
    "\n",
    "# Hinweis: Es wurde nur das Matching aktualisiert: statt rein exaktem Token-Abgleich\n",
    "# erfolgt nun zusätzliche Fuzzy-Suche via TF-IDF/Cosine (cutoff=0.70).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c5f0c2a-afc5-4829-bf91-c90170e61fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substitutions: 0 Zeilen; medical: 0\n",
      "Deletions:     102 Zeilen; medical: 7\n",
      "Insertions:    64 Zeilen; medical: 2\n"
     ]
    }
   ],
   "source": [
    "# Ziel: Ergänzung der Result-CSV um Spalten für Cosine-Similarity-Score und Best-Match-Vokabel \n",
    "# sowie Ref- und Hyp-Token-Spalten.\n",
    "\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. WER-Token-JSON laden\n",
    "wer_data = json.load(open(\"wer_token_sources.json\", encoding=\"utf-8\"))\n",
    "\n",
    "# 2. Medizinisches Vokabular laden\n",
    "cleaned = pd.read_csv(\"lexikon_cleaned_ger_synonyms.csv\")\n",
    "noun_set = set(w for phrase in cleaned[\"only_nouns\"].dropna() for w in phrase.split())\n",
    "adj_set  = set(w for phrase in cleaned[\"adjectives\"].dropna()  for w in phrase.split())\n",
    "\n",
    "atc = pd.read_csv(\"lexikon_ATC-Bedeutung_final_noarticles.csv\")\n",
    "atc_terms = atc[\"ATC-Bedeutung_cleaned\"].dropna().tolist()\n",
    "\n",
    "ling = pd.read_csv(\"lexikon_deDE15LinguisticVariant_final_noarticles.csv\")\n",
    "comp_terms = ling[\"COMPONENT_cleaned\"].dropna().tolist()\n",
    "\n",
    "def tokenize_list(phrases):\n",
    "    toks = set()\n",
    "    for p in phrases:\n",
    "        for w in re.sub(r\"[^\\w\\säöüß]\", \" \", p.lower()).split():\n",
    "            if w and w.isalpha():\n",
    "                toks.add(w)\n",
    "    return toks\n",
    "\n",
    "med_vocab = sorted(noun_set.union(adj_set, tokenize_list(atc_terms), tokenize_list(comp_terms)))\n",
    "\n",
    "# 3. TF-IDF Vectorizer aufbauen\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2,4))\n",
    "X_vocab = vectorizer.fit_transform(med_vocab)\n",
    "\n",
    "# 4. Funktion für Best-Match und Score\n",
    "def best_match_and_score(token):\n",
    "    if token in med_vocab:\n",
    "        return token, 1.0\n",
    "    vec = vectorizer.transform([token])\n",
    "    sims = cosine_similarity(vec, X_vocab).flatten()\n",
    "    idx = sims.argmax()\n",
    "    return med_vocab[idx], sims[idx]\n",
    "\n",
    "# 5. Token-Listen extrahieren\n",
    "subs_rows, dels_rows, ins_rows = [], [], []\n",
    "for entry in wer_data:\n",
    "    for r, h in zip(entry.get(\"subs_ref_tokens\", []), entry.get(\"subs_hyp_tokens\", [])):\n",
    "        subs_rows.append((r, h))\n",
    "    for r in entry.get(\"del_ref_tokens\", []):\n",
    "        dels_rows.append((r, \"\")) \n",
    "    for h in entry.get(\"ins_hyp_tokens\", []):\n",
    "        ins_rows.append((\"\", h))\n",
    "\n",
    "# 6. DataFrames aufbauen\n",
    "df_subs = pd.DataFrame(subs_rows, columns=[\"ref_token\", \"hyp_token\"])\n",
    "df_dels = pd.DataFrame(dels_rows, columns=[\"ref_token\", \"hyp_token\"])\n",
    "df_ins  = pd.DataFrame(ins_rows,  columns=[\"ref_token\", \"hyp_token\"])\n",
    "\n",
    "\n",
    "\n",
    "# 7. Zeilenweise Best-Match + Score ermitteln\n",
    "for df in (df_subs, df_dels, df_ins):\n",
    "    # Wähle pro Zeile das Token aus Ref oder Hyp\n",
    "    tokens = df.apply(\n",
    "        lambda row: row[\"ref_token\"] if row[\"ref_token\"] else row[\"hyp_token\"],\n",
    "        axis=1\n",
    "    )\n",
    "    # Wende best_match_and_score an\n",
    "    matches = tokens.apply(best_match_and_score)\n",
    "    # In separaten Series entpacken\n",
    "    df[\"best_match\"]    = matches.apply(lambda x: x[0])\n",
    "    df[\"cosine_score\"]  = matches.apply(lambda x: x[1])\n",
    "    # Markiere medizinisch ab Score ≥ 0.70\n",
    "    df[\"is_medical\"]    = df[\"cosine_score\"] >= 0.70\n",
    "\n",
    "# 8. Ergebnisse speichern\n",
    "df_subs.to_csv(\"result_subs_medical.csv\", index=False)\n",
    "df_dels.to_csv(\"result_dels_medical.csv\", index=False)\n",
    "df_ins .to_csv(\"result_ins_medical.csv\", index=False)\n",
    "\n",
    "print(f\"Substitutions: {len(df_subs)} Zeilen; medical: {df_subs['is_medical'].sum()}\")\n",
    "print(f\"Deletions:     {len(df_dels)} Zeilen; medical: {df_dels['is_medical'].sum()}\")\n",
    "print(f\"Insertions:    {len(df_ins)} Zeilen; medical: {df_ins['is_medical'].sum()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e376500f-0ba1-4490-8ea7-d8b718c91d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exports fertig: result_subs_medical.csv, result_dels_medical.csv, result_ins_medical.csv\n"
     ]
    }
   ],
   "source": [
    "# Ziel: Vollständiger Workflow mit Padding der Listen, damit alle DataFrame-Spalten gleiche Länge haben.\n",
    "\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. WER-Token-Quellen laden\n",
    "wer_data = json.load(open(\"wer_token_sources.json\", encoding=\"utf-8\"))\n",
    "\n",
    "# 2. Medizinisches Vokabular aus Lexika laden\n",
    "cleaned = pd.read_csv(\"lexikon_cleaned_ger_synonyms.csv\")\n",
    "noun_set = set(w for phrase in cleaned[\"only_nouns\"].dropna() for w in phrase.split())\n",
    "adj_set  = set(w for phrase in cleaned[\"adjectives\"].dropna()  for w in phrase.split())\n",
    "\n",
    "atc = pd.read_csv(\"lexikon_ATC-Bedeutung_final_noarticles.csv\")\n",
    "atc_terms = atc[\"ATC-Bedeutung_cleaned\"].dropna().tolist()\n",
    "\n",
    "ling = pd.read_csv(\"lexikon_deDE15LinguisticVariant_final_noarticles.csv\")\n",
    "comp_terms = ling[\"COMPONENT_cleaned\"].dropna().tolist()\n",
    "\n",
    "def tokenize_list(phrases):\n",
    "    toks = set()\n",
    "    for p in phrases:\n",
    "        for w in re.sub(r\"[^\\w\\säöüß]\", \" \", p.lower()).split():\n",
    "            if w and w.isalpha():\n",
    "                toks.add(w)\n",
    "    return toks\n",
    "\n",
    "atc_set  = tokenize_list(atc_terms)\n",
    "comp_set = tokenize_list(comp_terms)\n",
    "\n",
    "med_vocab = sorted(noun_set.union(adj_set, atc_set, comp_set))\n",
    "\n",
    "# 3. TF-IDF Vectorizer vorbereiten\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2,4))\n",
    "X_vocab = vectorizer.fit_transform(med_vocab)\n",
    "\n",
    "# 4. Helper-Funktion\n",
    "def best_match_and_score(token):\n",
    "    if token in med_vocab:\n",
    "        return token, 1.0\n",
    "    vec = vectorizer.transform([token])\n",
    "    sims = cosine_similarity(vec, X_vocab).flatten()\n",
    "    idx = sims.argmax()\n",
    "    return med_vocab[idx], sims[idx]\n",
    "\n",
    "# 5. Token-Listen sammeln\n",
    "subs_ref = []\n",
    "subs_hyp = []\n",
    "dels_ref = []\n",
    "ins_hyp  = []\n",
    "\n",
    "for entry in wer_data:\n",
    "    subs_ref += entry.get(\"subs_ref_tokens\", [])\n",
    "    subs_hyp += entry.get(\"subs_hyp_tokens\", [])\n",
    "    dels_ref += entry.get(\"del_ref_tokens\", [])\n",
    "    ins_hyp  += entry.get(\"ins_hyp_tokens\", [])\n",
    "\n",
    "# 6. Padding der Listen auf gleiche Länge\n",
    "def pad_list(lst, length):\n",
    "    return lst + [\"\"]*(length - len(lst))\n",
    "\n",
    "# Substitutions\n",
    "max_sub = max(len(subs_ref), len(subs_hyp))\n",
    "subs_ref = pad_list(subs_ref, max_sub)\n",
    "subs_hyp = pad_list(subs_hyp, max_sub)\n",
    "# Deletions (pad hyp)\n",
    "dels_ref = pad_list(dels_ref, len(dels_ref))\n",
    "dels_hyp = [\"\"] * len(dels_ref)\n",
    "# Insertions (pad ref)\n",
    "ins_ref = [\"\"] * len(ins_hyp)\n",
    "ins_hyp = pad_list(ins_hyp, len(ins_hyp))\n",
    "\n",
    "# 7. DataFrames erstellen\n",
    "df_subs = pd.DataFrame({\"ref_token\": subs_ref, \"hyp_token\": subs_hyp})\n",
    "df_dels = pd.DataFrame({\"ref_token\": dels_ref, \"hyp_token\": dels_hyp})\n",
    "df_ins  = pd.DataFrame({\"ref_token\": ins_ref, \"hyp_token\": ins_hyp})\n",
    "\n",
    "# 8. Matching und Score\n",
    "for df in (df_subs, df_dels, df_ins):\n",
    "    tokens = df[\"ref_token\"].where(df[\"ref_token\"]!=\"\", df[\"hyp_token\"])\n",
    "    matches = tokens.apply(best_match_and_score)\n",
    "    df[\"best_match\"]   = matches.apply(lambda x: x[0])\n",
    "    df[\"cosine_score\"] = matches.apply(lambda x: x[1])\n",
    "    df[\"is_medical\"]   = df[\"cosine_score\"] >= 0.80\n",
    "\n",
    "# 9. Export\n",
    "df_subs.to_csv(\"result_subs_medical.csv\", index=False)\n",
    "df_dels.to_csv(\"result_dels_medical.csv\", index=False)\n",
    "df_ins .to_csv(\"result_ins_medical.csv\", index=False)\n",
    "\n",
    "print(\"Exports fertig: result_subs_medical.csv, result_dels_medical.csv, result_ins_medical.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e323c6d6-6896-4c1f-a44c-84838c714c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substitutions: 615 Wörter, 60 medizinisch\n",
      "Deletions:     217 Wörter, 15 medizinisch\n",
      "Insertions:    259 Wörter, 22 medizinisch\n"
     ]
    }
   ],
   "source": [
    "# Ziel: Für jedes einzelne Wort aus Substitutions-, Deletions- und Insertions-Phrasen\n",
    "# fuzzy-matchen wir gegen das medizinische Vokabular und speichern das Ergebnis in CSVs.\n",
    "\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. WER-Token-Quellen laden\n",
    "wer_data = json.load(open(\"wer_token_sources.json\", encoding=\"utf-8\"))\n",
    "\n",
    "# 2. Medizinisches Vokabular aus den Lexika laden\n",
    "cleaned = pd.read_csv(\"lexikon_cleaned_ger_synonyms.csv\")\n",
    "noun_set = set(w for phrase in cleaned[\"only_nouns\"].dropna() for w in phrase.split())\n",
    "adj_set  = set(w for phrase in cleaned[\"adjectives\"].dropna()  for w in phrase.split())\n",
    "\n",
    "atc = pd.read_csv(\"lexikon_ATC-Bedeutung_final_noarticles.csv\")\n",
    "atc_terms = atc[\"ATC-Bedeutung_cleaned\"].dropna().tolist()\n",
    "\n",
    "ling = pd.read_csv(\"lexikon_deDE15LinguisticVariant_final_noarticles.csv\")\n",
    "comp_terms = ling[\"COMPONENT_cleaned\"].dropna().tolist()\n",
    "\n",
    "def tokenize_list(phrases):\n",
    "    toks = set()\n",
    "    for p in phrases:\n",
    "        for w in re.sub(r\"[^\\w\\säöüß]\", \" \", p.lower()).split():\n",
    "            if w and w.isalpha():\n",
    "                toks.add(w)\n",
    "    return toks\n",
    "\n",
    "atc_set  = tokenize_list(atc_terms)\n",
    "comp_set = tokenize_list(comp_terms)\n",
    "\n",
    "med_vocab = sorted(noun_set.union(adj_set, atc_set, comp_set))\n",
    "\n",
    "# 3. TF-IDF Vectorizer zur Fuzzy-Matching–Vorbereitung\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2,4))\n",
    "X_vocab = vectorizer.fit_transform(med_vocab)\n",
    "\n",
    "# 4. Hilfsfunktion: best_match + cosine_score für ein Token\n",
    "def best_match_and_score(token):\n",
    "    # Exakter Treffer\n",
    "    if token in med_vocab:\n",
    "        return token, 1.0\n",
    "    # Fuzzy-Matching\n",
    "    vec = vectorizer.transform([token])\n",
    "    sims = cosine_similarity(vec, X_vocab).flatten()\n",
    "    idx = sims.argmax()\n",
    "    return med_vocab[idx], sims[idx]\n",
    "\n",
    "# 5. Einzelwörter aus allen WER-Operationen sammeln\n",
    "subs_words = []\n",
    "dels_words = []\n",
    "ins_words  = []\n",
    "\n",
    "for entry in wer_data:\n",
    "    for phrase in entry.get(\"subs_ref_tokens\", []):\n",
    "        subs_words += [w for w in phrase.split()]\n",
    "    for phrase in entry.get(\"del_ref_tokens\", []):\n",
    "        dels_words += [w for w in phrase.split()]\n",
    "    for phrase in entry.get(\"ins_hyp_tokens\", []):\n",
    "        ins_words  += [w for w in phrase.split()]\n",
    "\n",
    "# 6. DataFrames erstellen und fuzzy matchen\n",
    "def build_df(tokens, label):\n",
    "    df = pd.DataFrame({\"token\": tokens})\n",
    "    df[\"best_match\"], df[\"cosine_score\"] = zip(*df[\"token\"].apply(best_match_and_score))\n",
    "    df[\"is_medical\"] = df[\"cosine_score\"] >= 0.80\n",
    "    return df\n",
    "\n",
    "df_subs = build_df(subs_words, \"substitution\")\n",
    "df_dels = build_df(dels_words, \"deletion\")\n",
    "df_ins  = build_df(ins_words,  \"insertion\")\n",
    "\n",
    "# 7. CSVs exportieren\n",
    "df_subs.to_csv(\"result_subs_medical.csv\", index=False)\n",
    "df_dels.to_csv(\"result_dels_medical.csv\", index=False)\n",
    "df_ins .to_csv(\"result_ins_medical.csv\", index=False)\n",
    "\n",
    "# 8. Kurze Zusammenfassung\n",
    "print(f\"Substitutions: {len(df_subs)} Wörter, {df_subs['is_medical'].sum()} medizinisch\")\n",
    "print(f\"Deletions:     {len(df_dels)} Wörter, {df_dels['is_medical'].sum()} medizinisch\")\n",
    "print(f\"Insertions:    {len(df_ins)} Wörter, {df_ins['is_medical'].sum()} medizinisch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e823ce46-4c99-4aa8-b4e3-8af335aa2049",
   "metadata": {},
   "source": [
    "# Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b25df3a-7fe6-4f77-ade3-9d0f985a5ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 132\u001b[39m\n\u001b[32m    129\u001b[39m hyp = doc.get(\u001b[33m\"\u001b[39m\u001b[33mtext_wer_denis\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Kennzahlen berechnen\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m scores = \u001b[43mcompute_wer_mwer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.80\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m row    = {**meta, **scores}\n\u001b[32m    134\u001b[39m results.append(row)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mcompute_wer_mwer\u001b[39m\u001b[34m(ref, hyp, cutoff)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# 5) m-WER: Nenner = Anzahl medizinischer Wörter im Ref\u001b[39;00m\n\u001b[32m     96\u001b[39m ref_tokens = ref.split()\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m total_med_ref = \u001b[38;5;28msum\u001b[39m(is_med(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m ref_tokens)\n\u001b[32m     98\u001b[39m mwer = (S_med + D_med + I_med) / total_med_ref \u001b[38;5;28;01mif\u001b[39;00m total_med_ref \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    101\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwer\u001b[39m\u001b[33m\"\u001b[39m: wer_score,\n\u001b[32m    102\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m: S, \u001b[33m\"\u001b[39m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m: D, \u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m: I,\n\u001b[32m    103\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mS_med\u001b[39m\u001b[33m\"\u001b[39m: S_med, \u001b[33m\"\u001b[39m\u001b[33mD_med\u001b[39m\u001b[33m\"\u001b[39m: D_med, \u001b[33m\"\u001b[39m\u001b[33mI_med\u001b[39m\u001b[33m\"\u001b[39m: I_med,\n\u001b[32m    104\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmwer\u001b[39m\u001b[33m\"\u001b[39m: mwer\n\u001b[32m    105\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# 5) m-WER: Nenner = Anzahl medizinischer Wörter im Ref\u001b[39;00m\n\u001b[32m     96\u001b[39m ref_tokens = ref.split()\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m total_med_ref = \u001b[38;5;28msum\u001b[39m(\u001b[43mis_med\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m ref_tokens)\n\u001b[32m     98\u001b[39m mwer = (S_med + D_med + I_med) / total_med_ref \u001b[38;5;28;01mif\u001b[39;00m total_med_ref \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    101\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwer\u001b[39m\u001b[33m\"\u001b[39m: wer_score,\n\u001b[32m    102\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m: S, \u001b[33m\"\u001b[39m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m: D, \u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m: I,\n\u001b[32m    103\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mS_med\u001b[39m\u001b[33m\"\u001b[39m: S_med, \u001b[33m\"\u001b[39m\u001b[33mD_med\u001b[39m\u001b[33m\"\u001b[39m: D_med, \u001b[33m\"\u001b[39m\u001b[33mI_med\u001b[39m\u001b[33m\"\u001b[39m: I_med,\n\u001b[32m    104\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmwer\u001b[39m\u001b[33m\"\u001b[39m: mwer\n\u001b[32m    105\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mcompute_wer_mwer.<locals>.is_med\u001b[39m\u001b[34m(tok)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_med\u001b[39m(tok):\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     match, score = \u001b[43mbest_match_and_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m score >= cutoff\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mbest_match_and_score\u001b[39m\u001b[34m(token)\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m token, \u001b[32m1.0\u001b[39m\n\u001b[32m     44\u001b[39m vec  = vectorizer.transform([token])\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m sims = \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_vocab\u001b[49m\u001b[43m)\u001b[49m.flatten()\n\u001b[32m     46\u001b[39m idx  = sims.argmax()\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m med_vocab[idx], sims[idx]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/speechbrain-fix/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/speechbrain-fix/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:1747\u001b[39m, in \u001b[36mcosine_similarity\u001b[39m\u001b[34m(X, Y, dense_output)\u001b[39m\n\u001b[32m   1745\u001b[39m     Y_normalized = X_normalized\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     Y_normalized = \u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m K = safe_sparse_dot(X_normalized, Y_normalized.T, dense_output=dense_output)\n\u001b[32m   1751\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m K\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/speechbrain-fix/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:189\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m func_sig = signature(func)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/speechbrain-fix/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:1965\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(X, norm, axis, copy, return_norm)\u001b[39m\n\u001b[32m   1961\u001b[39m     sparse_format = \u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1963\u001b[39m xp, _ = get_namespace(X)\n\u001b[32m-> \u001b[39m\u001b[32m1965\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1967\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43msparse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthe normalize function\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_array_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m0\u001b[39m:\n\u001b[32m   1974\u001b[39m     X = X.T\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/speechbrain-fix/lib/python3.11/site-packages/sklearn/utils/validation.py:1014\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1012\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sp.issparse(array):\n\u001b[32m   1013\u001b[39m     _ensure_no_complex_data(array)\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     array = \u001b[43m_ensure_sparse_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ensure_2d \u001b[38;5;129;01mand\u001b[39;00m array.ndim < \u001b[32m2\u001b[39m:\n\u001b[32m   1025\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1026\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected 2D input, got input with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1027\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1028\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1029\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mif it contains a single sample.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1030\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/speechbrain-fix/lib/python3.11/site-packages/sklearn/utils/validation.py:640\u001b[39m, in \u001b[36m_ensure_sparse_format\u001b[39m\u001b[34m(sparse_container, accept_sparse, dtype, copy, ensure_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[39m\n\u001b[32m    637\u001b[39m     sparse_container = sparse_container.astype(dtype)\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m changed_format:\n\u001b[32m    639\u001b[39m     \u001b[38;5;66;03m# force copy\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     sparse_container = \u001b[43msparse_container\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m    643\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(sparse_container, \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/speechbrain-fix/lib/python3.11/site-packages/scipy/sparse/_data.py:95\u001b[39m, in \u001b[36m_data_matrix.copy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._with_data(\u001b[38;5;28mself\u001b[39m.data.copy(), copy=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 0. (Einmalig) Dependencies installieren, falls nötig:\n",
    "# !pip install jiwer pymongo pandas scikit-learn\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from jiwer import process_words, collect_error_counts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Vorbereitung: medizinisches Vokabular laden & TF-IDF-Vectorizer fitten\n",
    "# ---------------------------------------------------------------------\n",
    "# a) Synonym-Korpus\n",
    "cleaned = pd.read_csv(\"lexikon_cleaned_ger_synonyms.csv\")\n",
    "noun_set = set(w for phrase in cleaned[\"only_nouns\"].dropna()  for w in phrase.split())\n",
    "adj_set  = set(w for phrase in cleaned[\"adjectives\"].dropna()   for w in phrase.split())\n",
    "\n",
    "# b) ATC- & Prozedur-Lexika\n",
    "atc   = pd.read_csv(\"lexikon_ATC-Bedeutung_final_noarticles.csv\")\n",
    "ling  = pd.read_csv(\"lexikon_deDE15LinguisticVariant_final_noarticles.csv\")\n",
    "\n",
    "def tokenize_list(phrases):\n",
    "    toks = set()\n",
    "    for p in phrases:\n",
    "        for w in re.sub(r\"[^\\w\\säöüß]\", \" \", str(p).lower()).split():\n",
    "            if w.isalpha():\n",
    "                toks.add(w)\n",
    "    return toks\n",
    "\n",
    "atc_set  = tokenize_list(atc[\"ATC-Bedeutung_cleaned\"].dropna())\n",
    "comp_set = tokenize_list(ling[\"COMPONENT_cleaned\"].dropna())\n",
    "\n",
    "# c) Gesamt-Vokabular und TF-IDF Vectorizer\n",
    "med_vocab = sorted(noun_set.union(adj_set, atc_set, comp_set))\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2,4))\n",
    "X_vocab = vectorizer.fit_transform(med_vocab)\n",
    "\n",
    "def best_match_and_score(token):\n",
    "    \"\"\"Gibt (best_match, cosine_score) zurück.\"\"\"\n",
    "    if token in med_vocab:\n",
    "        return token, 1.0\n",
    "    vec  = vectorizer.transform([token])\n",
    "    sims = cosine_similarity(vec, X_vocab).flatten()\n",
    "    idx  = sims.argmax()\n",
    "    return med_vocab[idx], sims[idx]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Funktion: WER- und m-WER-Berechnung für ein einzelnes Ref/Hyp-Paar\n",
    "# ---------------------------------------------------------------------\n",
    "def compute_wer_mwer(ref: str, hyp: str, cutoff=0.80):\n",
    "    # 1) WER-Alignment\n",
    "    out        = process_words(ref, hyp)\n",
    "    subs_d, ins_d, del_d = collect_error_counts(out)\n",
    "\n",
    "    # 2) Standard-WER-Werte\n",
    "    wer_score = out.wer\n",
    "    S = sum(subs_d.values())\n",
    "    D = sum(del_d.values())\n",
    "    I = sum(ins_d.values())\n",
    "\n",
    "    # 3) Einzelwörter extrahieren\n",
    "    # Bei Substitution: key = (ref_phrase, hyp_phrase)\n",
    "    subs_words = [\n",
    "        w\n",
    "        for (ref_phrase, _), cnt in subs_d.items()\n",
    "        for w in ref_phrase.split()\n",
    "        for _ in range(cnt)\n",
    "    ]\n",
    "    # Bei Deletion: key = ref_phrase\n",
    "    del_words = [\n",
    "        w\n",
    "        for ref_phrase, cnt in del_d.items()\n",
    "        for w in ref_phrase.split()\n",
    "        for _ in range(cnt)\n",
    "    ]\n",
    "    # Bei Insertion: key = hyp_phrase\n",
    "    ins_words = [\n",
    "        w\n",
    "        for hyp_phrase, cnt in ins_d.items()\n",
    "        for w in hyp_phrase.split()\n",
    "        for _ in range(cnt)\n",
    "    ]\n",
    "\n",
    "    # 4) Fuzzy-Medical-Check\n",
    "    def is_med(tok):\n",
    "        match, score = best_match_and_score(tok)\n",
    "        return score >= cutoff\n",
    "\n",
    "    S_med = sum(is_med(w) for w in subs_words)\n",
    "    D_med = sum(is_med(w) for w in del_words)\n",
    "    I_med = sum(is_med(w) for w in ins_words)\n",
    "\n",
    "    # 5) m-WER: Nenner = Anzahl medizinischer Wörter im Ref\n",
    "    ref_tokens = ref.split()\n",
    "    total_med_ref = sum(is_med(w) for w in ref_tokens)\n",
    "    mwer = (S_med + D_med + I_med) / total_med_ref if total_med_ref else 0.0\n",
    "\n",
    "    return {\n",
    "        \"wer\": wer_score,\n",
    "        \"S\": S, \"D\": D, \"I\": I,\n",
    "        \"S_med\": S_med, \"D_med\": D_med, \"I_med\": I_med,\n",
    "        \"mwer\": mwer\n",
    "    }\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Schleife über alle Transcripts, Auslesen der Meta-Felder und CSV-Export\n",
    "# ---------------------------------------------------------------------\n",
    "client = MongoClient(\"mongodb://localhost:27018/\")\n",
    "col    = client[\"transcriptions\"][\"transcripts_denis\"]\n",
    "\n",
    "results = []\n",
    "cursor  = col.find({\"excludeGeneral\": 0}, \n",
    "                   {\"convoID\":1, \"ambientVariant\":1, \"processedVolume\":1,\n",
    "                    \"technology\":1, \"model\":1,\n",
    "                    \"src_wer_denis\":1, \"text_wer_denis\":1})\n",
    "\n",
    "for doc in cursor:\n",
    "    meta = {\n",
    "        \"convoID\":        doc.get(\"convoID\"),\n",
    "        \"ambientVariant\": doc.get(\"ambientVariant\"),\n",
    "        \"processedVolume\":doc.get(\"processedVolume\"),\n",
    "        \"technology\":     doc.get(\"technology\"),\n",
    "        \"model\":          doc.get(\"model\")\n",
    "    }\n",
    "    # Ref/Hyp aus DB (bereits normalisiert)\n",
    "    ref = doc.get(\"src_wer_denis\", \"\")\n",
    "    hyp = doc.get(\"text_wer_denis\", \"\")\n",
    "\n",
    "    # Kennzahlen berechnen\n",
    "    scores = compute_wer_mwer(ref, hyp, cutoff=0.80)\n",
    "    row    = {**meta, **scores}\n",
    "    results.append(row)\n",
    "\n",
    "# In DataFrame und CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"transcripts_wer_mwer.csv\", index=False)\n",
    "\n",
    "print(f\"{len(df)} Transcripts verarbeitet und in 'transcripts_wer_mwer.csv' gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a99f8032-7ef0-484c-896c-6696042ffdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11880 Transcripts verarbeitet und in 'transcripts_wer_mwer_fast.csv' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# 0. (Einmalig) Dependencies installieren:\n",
    "# !pip install jiwer pymongo pandas scikit-learn\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from jiwer import process_words, collect_error_counts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Medizinisches Vokabular & TF-IDF-Vectorizer fitten\n",
    "# ---------------------------------------------------------------------\n",
    "cleaned = pd.read_csv(\"lexikon_cleaned_ger_synonyms.csv\")\n",
    "noun_set = set(w for ph in cleaned[\"only_nouns\"].dropna() for w in ph.split())\n",
    "adj_set  = set(w for ph in cleaned[\"adjectives\"].dropna()  for w in ph.split())\n",
    "\n",
    "atc   = pd.read_csv(\"lexikon_ATC-Bedeutung_final_noarticles.csv\")\n",
    "ling  = pd.read_csv(\"lexikon_deDE15LinguisticVariant_final_noarticles.csv\")\n",
    "\n",
    "def tokenize_list(phrases):\n",
    "    toks = set()\n",
    "    for p in phrases:\n",
    "        for w in re.sub(r\"[^\\w\\säöüß]\", \" \", str(p).lower()).split():\n",
    "            if w.isalpha(): toks.add(w)\n",
    "    return toks\n",
    "\n",
    "atc_set  = tokenize_list(atc[\"ATC-Bedeutung_cleaned\"].dropna())\n",
    "comp_set = tokenize_list(ling[\"COMPONENT_cleaned\"].dropna())\n",
    "\n",
    "med_vocab = sorted(noun_set.union(adj_set, atc_set, comp_set))\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2,4))\n",
    "X_vocab = vectorizer.fit_transform(med_vocab)\n",
    "\n",
    "def best_match_and_score(token):\n",
    "    if token in med_vocab:\n",
    "        return token, 1.0\n",
    "    vec  = vectorizer.transform([token])\n",
    "    sims = cosine_similarity(vec, X_vocab).flatten()\n",
    "    idx  = sims.argmax()\n",
    "    return med_vocab[idx], sims[idx]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Einmaliges Sammeln aller UNIQUE TOKENS aus wer_token_sources.json\n",
    "# ---------------------------------------------------------------------\n",
    "wer_data = json.load(open(\"wer_token_sources.json\", encoding=\"utf-8\"))\n",
    "unique_tokens = set()\n",
    "\n",
    "for entry in wer_data:\n",
    "    unique_tokens.update(token for phrase in entry.get(\"subs_ref_tokens\", []) for token in phrase.split())\n",
    "    unique_tokens.update(token for phrase in entry.get(\"del_ref_tokens\", [])   for token in phrase.split())\n",
    "    unique_tokens.update(token for phrase in entry.get(\"ins_hyp_tokens\", [])   for token in phrase.split())\n",
    "\n",
    "# Batch TF-IDF + Cosine\n",
    "tokens_list = sorted(unique_tokens)\n",
    "X_tok       = vectorizer.transform(tokens_list)\n",
    "sims_matrix = cosine_similarity(X_tok, X_vocab)\n",
    "\n",
    "# Lookup-Map aufbauen einmalig\n",
    "token_to_medinfo = {\n",
    "    tok: (med_vocab[idx], float(sims_matrix[i, idx]))\n",
    "    for i, tok in enumerate(tokens_list)\n",
    "    for idx in [sims_matrix[i].argmax()]\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Schnelle WER + m-WER Berechnung unter Einsatz des Lookup-Maps\n",
    "# ---------------------------------------------------------------------\n",
    "def compute_wer_mwer_fast(ref, hyp, cutoff=0.80):\n",
    "    out        = process_words(ref, hyp)\n",
    "    subs_d, ins_d, del_d = collect_error_counts(out)\n",
    "\n",
    "    # WER\n",
    "    wer_score = out.wer\n",
    "\n",
    "    # Hilfsfunktion für Count\n",
    "    def count_med(dict_ops, key_idx):\n",
    "        cnt = 0\n",
    "        for key, c in dict_ops.items():\n",
    "            # key_idx = 0 für Sub (ref side), None für Del/Ins (single-string)\n",
    "            phrase = key[0] if isinstance(key, tuple) and key_idx==0 else key if not isinstance(key, tuple) else key[1]\n",
    "            for w in phrase.split():\n",
    "                if token_to_medinfo.get(w, (\"\",0.0))[1] >= cutoff:\n",
    "                    cnt += 1\n",
    "        return cnt\n",
    "\n",
    "    S_med = count_med(subs_d, 0)\n",
    "    D_med = count_med(del_d, None)\n",
    "    I_med = count_med(ins_d, None)\n",
    "\n",
    "    # m-WER Nenner: medizinische Wörter im Ref\n",
    "    total_med_ref = sum(\n",
    "        1 for w in ref.split()\n",
    "        if token_to_medinfo.get(w, (\"\",0.0))[1] >= cutoff\n",
    "    )\n",
    "    mwer = (S_med + D_med + I_med) / total_med_ref if total_med_ref else 0.0\n",
    "\n",
    "    return wer_score, S_med, D_med, I_med, mwer\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Über alle Transcripts iterieren & CSV schreiben\n",
    "# ---------------------------------------------------------------------\n",
    "client = MongoClient(\"mongodb://localhost:27018/\")\n",
    "col = client[\"transcriptions\"][\"transcripts_denis\"]\n",
    "\n",
    "cursor = col.find(\n",
    "    {\"excludeGeneral\": 0},\n",
    "    {\"convoID\":1, \"ambientVariant\":1, \"processedVolume\":1,\n",
    "     \"technology\":1, \"model\":1,\n",
    "     \"src_wer_denis\":1, \"text_wer_denis\":1}\n",
    ")\n",
    "\n",
    "rows = []\n",
    "for doc in cursor:\n",
    "    wer, S_med, D_med, I_med, mwer = compute_wer_mwer_fast(\n",
    "        doc[\"src_wer_denis\"], doc[\"text_wer_denis\"], cutoff=0.80\n",
    "    )\n",
    "    rows.append({\n",
    "        \"convoID\":         doc.get(\"convoID\"),\n",
    "        \"ambientVariant\":  doc.get(\"ambientVariant\"),\n",
    "        \"processedVolume\": doc.get(\"processedVolume\"),\n",
    "        \"technology\":      doc.get(\"technology\"),\n",
    "        \"model\":           doc.get(\"model\"),\n",
    "        \"wer\":             wer,\n",
    "        \"S_med\":           S_med,\n",
    "        \"D_med\":           D_med,\n",
    "        \"I_med\":           I_med,\n",
    "        \"mwer\":            mwer\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"transcripts_wer_mwer_fast.csv\", index=False)\n",
    "print(f\"{len(df)} Transcripts verarbeitet und in 'transcripts_wer_mwer_fast.csv' gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "25ba3860-faef-490f-bff1-cb21023315db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11880 Transcripts verarbeitet und in 'transcripts_wer_mwer_fast.csv' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# Ziel: Erweiterung des schnellen Workflows um Standard-WER-Counts (S, D, I) und Ausgabe in CSV\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from jiwer import process_words, collect_error_counts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. Medizinisches Vokabular & TF-IDF einrichten (wie zuvor)\n",
    "cleaned = pd.read_csv(\"lexikon_cleaned_ger_synonyms.csv\")\n",
    "noun_set = set(w for ph in cleaned[\"only_nouns\"].dropna() for w in ph.split())\n",
    "adj_set  = set(w for ph in cleaned[\"adjectives\"].dropna()  for w in ph.split())\n",
    "atc   = pd.read_csv(\"lexikon_ATC-Bedeutung_final_noarticles.csv\")\n",
    "ling  = pd.read_csv(\"lexikon_deDE15LinguisticVariant_final_noarticles.csv\")\n",
    "\n",
    "def tokenize_list(phrases):\n",
    "    toks = set()\n",
    "    for p in phrases:\n",
    "        for w in re.sub(r\"[^\\w\\säöüß]\", \" \", str(p).lower()).split():\n",
    "            if w.isalpha(): toks.add(w)\n",
    "    return toks\n",
    "\n",
    "med_vocab = sorted(noun_set.union(adj_set,\n",
    "                                  tokenize_list(atc[\"ATC-Bedeutung_cleaned\"].dropna()),\n",
    "                                  tokenize_list(ling[\"COMPONENT_cleaned\"].dropna())))\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2,4))\n",
    "X_vocab = vectorizer.fit_transform(med_vocab)\n",
    "\n",
    "# 2. Fuzzy-Matching Lookup vorbereiten\n",
    "wer_data = json.load(open(\"wer_token_sources.json\", encoding=\"utf-8\"))\n",
    "unique_tokens = set()\n",
    "for e in wer_data:\n",
    "    unique_tokens.update(w for ph in e.get(\"subs_ref_tokens\", []) for w in ph.split())\n",
    "    unique_tokens.update(w for ph in e.get(\"del_ref_tokens\", [])   for w in ph.split())\n",
    "    unique_tokens.update(w for ph in e.get(\"ins_hyp_tokens\", [])   for w in ph.split())\n",
    "\n",
    "tokens_list = sorted(unique_tokens)\n",
    "X_tok = vectorizer.transform(tokens_list)\n",
    "sims_matrix = cosine_similarity(X_tok, X_vocab)\n",
    "token_to_medinfo = {\n",
    "    tok: (med_vocab[idx], float(sims_matrix[i, idx]))\n",
    "    for i, tok in enumerate(tokens_list)\n",
    "    for idx in [sims_matrix[i].argmax()]\n",
    "}\n",
    "\n",
    "# 3. Schnelle Function inkl. Standard-WER Counts\n",
    "def compute_wer_mwer_fast(ref, hyp, cutoff=0.80):\n",
    "    out        = process_words(ref, hyp)\n",
    "    subs_d, ins_d, del_d = collect_error_counts(out)\n",
    "    wer_score = out.wer\n",
    "    S = sum(subs_d.values())\n",
    "    D = sum(del_d.values())\n",
    "    I = sum(ins_d.values())\n",
    "\n",
    "    def is_med(tok):\n",
    "        return token_to_medinfo.get(tok, (\"\",0.0))[1] >= cutoff\n",
    "\n",
    "    S_med = sum(is_med(w) for (ref_ph, _), cnt in subs_d.items() for w in ref_ph.split() for _ in range(cnt))\n",
    "    D_med = sum(is_med(w) for ph, cnt in del_d.items() for w in ph.split() for _ in range(cnt))\n",
    "    I_med = sum(is_med(w) for ph, cnt in ins_d.items() for w in ph.split() for _ in range(cnt))\n",
    "\n",
    "    total_med_ref = sum(is_med(w) for w in ref.split())\n",
    "    mwer = (S_med + D_med + I_med) / total_med_ref if total_med_ref else 0.0\n",
    "\n",
    "    return wer_score, S, D, I, S_med, D_med, I_med, mwer\n",
    "\n",
    "# 4. Über alle Transcripts iterieren und CSV schreiben\n",
    "client = MongoClient(\"mongodb://localhost:27018/\")\n",
    "col = client[\"transcriptions\"][\"transcripts_denis\"]\n",
    "cursor = col.find(\n",
    "    {\"excludeGeneral\": 0},\n",
    "    {\"convoID\":1, \"ambientVariant\":1, \"processedVolume\":1,\n",
    "     \"technology\":1, \"model\":1,\n",
    "     \"src_wer_denis\":1, \"text_wer_denis\":1}\n",
    ")\n",
    "\n",
    "rows = []\n",
    "for doc in cursor:\n",
    "    wer, S, D, I, S_med, D_med, I_med, mwer = compute_wer_mwer_fast(\n",
    "        doc[\"src_wer_denis\"], doc[\"text_wer_denis\"], cutoff=0.80\n",
    "    )\n",
    "    rows.append({\n",
    "        \"convoID\":        doc.get(\"convoID\"),\n",
    "        \"ambientVariant\": doc.get(\"ambientVariant\"),\n",
    "        \"processedVolume\":doc.get(\"processedVolume\"),\n",
    "        \"technology\":     doc.get(\"technology\"),\n",
    "        \"model\":          doc.get(\"model\"),\n",
    "        \"wer\":            wer,\n",
    "        \"S\":              S,\n",
    "        \"D\":              D,\n",
    "        \"I\":              I,\n",
    "        \"S_med\":          S_med,\n",
    "        \"D_med\":          D_med,\n",
    "        \"I_med\":          I_med,\n",
    "        \"mwer\":           mwer\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"transcripts_wer_mwer_fast.csv\", index=False)\n",
    "print(f\"{len(df)} Transcripts verarbeitet und in 'transcripts_wer_mwer_fast.csv' gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6090d10-1e56-4f8f-801d-2598a2c97f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11880 Transcripts verarbeitet. Ergebnis in 'transcripts_wer_mwer_phrase.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Ziel: Phrase-Level m-WER, bei dem jede WER-Operation als _med_ zählt, \n",
    "# wenn eine der enthaltenen Wörter medizinisch ist (Cutoff ≥ 0.80).\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from jiwer import process_words, collect_error_counts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Medizinisches Vokabular vorbereiten\n",
    "# -------------------------------\n",
    "cleaned = pd.read_csv(\"lexikon_cleaned_ger_synonyms.csv\")\n",
    "noun_set = set(w for ph in cleaned[\"only_nouns\"].dropna() for w in ph.split())\n",
    "adj_set  = set(w for ph in cleaned[\"adjectives\"].dropna()  for w in ph.split())\n",
    "\n",
    "atc   = pd.read_csv(\"lexikon_ATC-Bedeutung_final_noarticles.csv\")\n",
    "ling  = pd.read_csv(\"lexikon_deDE15LinguisticVariant_final_noarticles.csv\")\n",
    "\n",
    "def tokenize_list(phrases):\n",
    "    toks = set()\n",
    "    for p in phrases:\n",
    "        for w in re.sub(r\"[^\\w\\säöüß]\", \" \", str(p).lower()).split():\n",
    "            if w.isalpha(): toks.add(w)\n",
    "    return toks\n",
    "\n",
    "atc_set  = tokenize_list(atc[\"ATC-Bedeutung_cleaned\"].dropna())\n",
    "comp_set = tokenize_list(ling[\"COMPONENT_cleaned\"].dropna())\n",
    "\n",
    "med_vocab = sorted(noun_set.union(adj_set, atc_set, comp_set))\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2,4))\n",
    "X_vocab = vectorizer.fit_transform(med_vocab)\n",
    "\n",
    "# Batch-unique tokens from previous JSON (to build lookup)\n",
    "wer_data = json.load(open(\"wer_token_sources.json\", encoding=\"utf-8\"))\n",
    "unique_tokens = set()\n",
    "for e in wer_data:\n",
    "    unique_tokens.update(w for ph in e.get(\"subs_ref_tokens\", []) for w in ph.split())\n",
    "    unique_tokens.update(w for ph in e.get(\"del_ref_tokens\", [])   for w in ph.split())\n",
    "    unique_tokens.update(w for ph in e.get(\"ins_hyp_tokens\", [])   for w in ph.split())\n",
    "\n",
    "tokens_list = sorted(unique_tokens)\n",
    "X_tok = vectorizer.transform(tokens_list)\n",
    "sims_matrix = cosine_similarity(X_tok, X_vocab)\n",
    "token_to_score = {\n",
    "    tok: float(sims_matrix[i].max())\n",
    "    for i, tok in enumerate(tokens_list)\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Phrase-Level compute function\n",
    "# -------------------------------\n",
    "def compute_wer_mwer_phrase(ref, hyp, cutoff=0.80):\n",
    "    out       = process_words(ref, hyp)\n",
    "    subs_d, ins_d, del_d = collect_error_counts(out)\n",
    "    \n",
    "    # Standard counts\n",
    "    wer_score = out.wer\n",
    "    S = sum(subs_d.values())\n",
    "    D = sum(del_d.values())\n",
    "    I = sum(ins_d.values())\n",
    "\n",
    "    # Phrase-level medical counts\n",
    "    S_med = sum(\n",
    "        cnt for (ref_ph, _), cnt in subs_d.items()\n",
    "        if any(token_to_score.get(w,0.0) >= cutoff for w in ref_ph.split())\n",
    "    )\n",
    "    D_med = sum(\n",
    "        cnt for ref_ph, cnt in del_d.items()\n",
    "        if any(token_to_score.get(w,0.0) >= cutoff for w in ref_ph.split())\n",
    "    )\n",
    "    I_med = sum(\n",
    "        cnt for hyp_ph, cnt in ins_d.items()\n",
    "        if any(token_to_score.get(w,0.0) >= cutoff for w in hyp_ph.split())\n",
    "    )\n",
    "\n",
    "    # m-WER denominator: number of medical words in ref\n",
    "    ref_tokens = ref.split()\n",
    "    total_med_ref = sum(1 for w in ref_tokens if token_to_score.get(w,0.0) >= cutoff)\n",
    "    mwer = (S_med + D_med + I_med) / total_med_ref if total_med_ref else 0.0\n",
    "\n",
    "    return wer_score, S, D, I, S_med, D_med, I_med, mwer\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Über DB iterieren und CSV export\n",
    "# -------------------------------\n",
    "client = MongoClient(\"mongodb://localhost:27018/\")\n",
    "col = client[\"transcriptions\"][\"transcripts_denis\"]\n",
    "\n",
    "cursor = col.find(\n",
    "    {\"excludeGeneral\": 0},\n",
    "    {\"convoID\":1, \"ambientVariant\":1, \"processedVolume\":1,\n",
    "     \"technology\":1, \"model\":1,\n",
    "     \"src_wer_denis\":1, \"text_wer_denis\":1}\n",
    ")\n",
    "\n",
    "rows = []\n",
    "for doc in cursor:\n",
    "    ref = doc.get(\"src_wer_denis\",\"\")\n",
    "    hyp = doc.get(\"text_wer_denis\",\"\")\n",
    "    wer, S, D, I, S_med, D_med, I_med, mwer = compute_wer_mwer_phrase(ref, hyp)\n",
    "    rows.append({\n",
    "        \"convoID\":        doc.get(\"convoID\"),\n",
    "        \"ambientVariant\": doc.get(\"ambientVariant\"),\n",
    "        \"processedVolume\":doc.get(\"processedVolume\"),\n",
    "        \"technology\":     doc.get(\"technology\"),\n",
    "        \"model\":          doc.get(\"model\"),\n",
    "        \"wer\":            wer,\n",
    "        \"S\":              S,\n",
    "        \"D\":              D,\n",
    "        \"I\":              I,\n",
    "        \"S_med\":          S_med,\n",
    "        \"D_med\":          D_med,\n",
    "        \"I_med\":          I_med,\n",
    "        \"mwer\":           mwer\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"transcripts_wer_mwer_phrase.csv\", index=False)\n",
    "print(f\"{len(df)} Transcripts verarbeitet. Ergebnis in 'transcripts_wer_mwer_phrase.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789d924-c8fa-4371-8729-b19a8cb4b348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanic eval",
   "language": "python",
   "name": "stanic-eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
