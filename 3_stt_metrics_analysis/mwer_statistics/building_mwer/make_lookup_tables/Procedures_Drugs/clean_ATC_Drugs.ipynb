{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e618e059-6fb3-4376-a2a4-28cbb4f64ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet-Namen in der Datei: ['Urheberrrechte', 'amtl. Index 2025 ATC-sortiert', 'amtl. Index 2025 alphabetisch ', 'Erläuterungen pflanzliche ATC', 'Erläuterungen']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "xlsm_path = os.path.join(base_dir, 'Amtliche Fassung des ATC-Index 2025.xlsm')\n",
    "xls = pd.ExcelFile(xlsm_path, engine='openpyxl')\n",
    "print(\"Sheet-Namen in der Datei:\", xls.sheet_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71ae44b7-5660-4a62-aae3-c042a3c0250c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV mit Spalte 'ATC-Bedeutung' erstellt: /Users/mosimacnew/Code/speechbrain-fix/myNotebooks/2_STT-Metriken/mwer/make_korpus/Procedures_Drugs/ATC-Bedeutung.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Absoluter Pfad zum Arbeitsverzeichnis\n",
    "base_dir = '/Users/mosimacnew/Code/speechbrain-fix/myNotebooks/2_STT-Metriken/mwer/make_korpus/Procedures_Drugs'\n",
    "xlsm_path = os.path.join(base_dir, 'Amtliche Fassung des ATC-Index 2025.xlsm')\n",
    "\n",
    "# Sheet-Name exakt so wie in der Datei (inkl. Leerzeichen)\n",
    "sheet_name = 'amtl. Index 2025 alphabetisch '\n",
    "df = pd.read_excel(xlsm_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "\n",
    "# Nur die Spalte 'ATC-Bedeutung' auswählen\n",
    "col = 'ATC-Bedeutung'\n",
    "if col in df.columns:\n",
    "    df_col = df[[col]]\n",
    "    csv_path = os.path.join(base_dir, 'ATC-Bedeutung.csv')\n",
    "    df_col.to_csv(csv_path, sep=';', index=False, encoding='utf-8')\n",
    "    print(f\"CSV mit Spalte '{col}' erstellt: {csv_path}\")\n",
    "    df_col.head()\n",
    "else:\n",
    "    print(f\"Spalte '{col}' nicht gefunden. Verfügbare Spalten sind:\\n{df.columns.tolist()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac308547-aa96-4512-8510-acbeb6e7b578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Zeilen vor Entfernen der Duplikate: 7436\n",
      "Anzahl Zeilen nach Entfernen der Duplikate: 6027\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. CSV laden\n",
    "csv_path = 'ATC-Bedeutung.csv'  # passe den Pfad ggf. an\n",
    "df = pd.read_csv(csv_path, sep=';', encoding='utf-8')\n",
    "\n",
    "# 2. Zeilen zählen vor dem Bereinigen\n",
    "print(f\"Anzahl Zeilen vor Entfernen der Duplikate: {len(df)}\")\n",
    "\n",
    "# 3. Duplikate entfernen (alle Spalten)\n",
    "df_clean = df.drop_duplicates()\n",
    "\n",
    "# 4. Zeilen zählen nach dem Bereinigen\n",
    "print(f\"Anzahl Zeilen nach Entfernen der Duplikate: {len(df_clean)}\")\n",
    "\n",
    "# 5. Optional: in eine neue CSV schreiben\n",
    "df_clean.to_csv('ATC-Bedeutung_dedup.csv', sep=';', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b62e300-da64-475e-9a28-d3c1a30a3fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ATC-Bedeutung ATC-Bedeutung_cleaned\n",
      "0             NaN                   NaN\n",
      "1        Abacavir              abacavir\n",
      "2    Abaloparatid          abaloparatid\n",
      "3      Abametapir            abametapir\n",
      "4        Abarelix              abarelix\n",
      "5       Abatacept             abatacept\n",
      "6       Abciximab             abciximab\n",
      "7     Abemaciclib           abemaciclib\n",
      "8        Abetimus              abetimus\n",
      "9  Abicipar pegol        abicipar pegol\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Arbeitsverzeichnis ermitteln\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# 2. Pfad zur deduplizierten CSV\n",
    "csv_path = os.path.join(base_dir, 'ATC-Bedeutung_dedup.csv')\n",
    "\n",
    "# 3. CSV laden\n",
    "df = pd.read_csv(csv_path, sep=';', encoding='utf-8')\n",
    "\n",
    "# 2. Funktion zur Textbereinigung\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # in Kleinbuchstaben\n",
    "    text = text.lower()\n",
    "    # Umlaute ersetzen\n",
    "    umlaut_map = {\n",
    "        'ä': 'ae', 'ö': 'oe', 'ü': 'ue',\n",
    "        'Ä': 'Ae', 'Ö': 'Oe', 'Ü': 'Ue', 'ß': 'ss'\n",
    "    }\n",
    "    for uml, rep in umlaut_map.items():\n",
    "        text = text.replace(uml, rep)\n",
    "    # Bindestriche und Klammern durch Leerzeichen\n",
    "    text = text.replace('-', ' ')\n",
    "    text = text.replace('(', ' ').replace(')', ' ')\n",
    "    # Stop-Wörter entfernen (als eigene Worte)\n",
    "    stopw = ['oder', 'und', 'in']\n",
    "    for w in stopw:\n",
    "        text = re.sub(r'\\b' + w + r'\\b', ' ', text)\n",
    "    # Alle Sonderzeichen, Interpunktion und Ziffern entfernen\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    # Mehrfache Leerzeichen auf Einfach-Leerzeichen reduzieren\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# 3. Neue Spalte anlegen\n",
    "df['ATC-Bedeutung_cleaned'] = df['ATC-Bedeutung'].apply(clean_text)\n",
    "\n",
    "# 4. Ergebnis prüfen\n",
    "print(df[['ATC-Bedeutung', 'ATC-Bedeutung_cleaned']].head(10))\n",
    "\n",
    "# 5. Optional: speichern\n",
    "df.to_csv('ATC-Bedeutung_cleaned.csv', sep=';', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49a9c83e-e94c-4715-8c05-92307cb4aee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeilen vor Bereinigung: 6027\n",
      "Zeilen nach Entfernen leerer Zeilen: 6026\n",
      "Bereinigte CSV gespeichert: /Users/mosimacnew/Code/speechbrain-fix/myNotebooks/2_STT-Metriken/mwer/make_korpus/Procedures_Drugs/ATC-Bedeutung_final.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Arbeitsverzeichnis ermitteln\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# 2. Pfad zur bisherigen semikolon-getrennten CSV\n",
    "csv_in = os.path.join(base_dir, 'ATC-Bedeutung_cleaned.csv')\n",
    "\n",
    "# 3. Einlesen (Semikolon als Trenner)\n",
    "df = pd.read_csv(csv_in, sep=';', encoding='utf-8')\n",
    "\n",
    "# 4. Count vor dem Entfernen leerer Zeilen\n",
    "print(f\"Zeilen vor Bereinigung: {len(df)}\")\n",
    "\n",
    "# 5. Alle Zeilen entfernen, die in allen Spalten NaN sind\n",
    "df_clean = df.dropna(how='all')\n",
    "\n",
    "# 6. Count nachher\n",
    "print(f\"Zeilen nach Entfernen leerer Zeilen: {len(df_clean)}\")\n",
    "\n",
    "# 7. Abspeichern mit Komma-Delimiter (Standard)\n",
    "csv_out = os.path.join(base_dir, 'ATC-Bedeutung_final.csv')\n",
    "df_clean.to_csv(csv_out, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Bereinigte CSV gespeichert: {csv_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcc195ed-3515-4bcb-8385-a675571cec3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neues CSV erstellt: /Users/mosimacnew/Code/speechbrain-fix/myNotebooks/2_STT-Metriken/mwer/make_korpus/Procedures_Drugs/deDE15LinguisticVariant_final.csv (18257 Zeilen)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Arbeitsverzeichnis ermitteln\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# 2. Eingangs-CSV und Ausgangspfade\n",
    "csv_in  = os.path.join(base_dir, 'deDE15LinguisticVariant.csv')\n",
    "csv_out = os.path.join(base_dir, 'deDE15LinguisticVariant_final.csv')\n",
    "\n",
    "# 3. Nur die Spalte COMPONENT einlesen\n",
    "df = pd.read_csv(csv_in, usecols=['COMPONENT'], encoding='utf-8')\n",
    "\n",
    "# 4. Funktion zur Textbereinigung für COMPONENT_cleaned\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # 1) Kleinbuchstaben\n",
    "    text = text.lower()\n",
    "    # 2) Umlaute ersetzen\n",
    "    umlaut_map = {'ä':'ae','ö':'oe','ü':'ue','ß':'ss'}\n",
    "    for uml, rep in umlaut_map.items():\n",
    "        text = text.replace(uml, rep)\n",
    "    # 3) Stop-Wörter entfernen\n",
    "    for w in ['oder','und','in']:\n",
    "        text = re.sub(r'\\b{}\\b'.format(w), ' ', text)\n",
    "    # 4) \"XXX\" entfernen\n",
    "    text = text.replace('xxx', ' ')\n",
    "    # 5) Bindestriche und Klammern durch Leerzeichen\n",
    "    text = text.replace('-', ' ')\n",
    "    text = text.replace('(', ' ').replace(')', ' ')\n",
    "    # 6) Sonderzeichen, Interpunktion, Zahlen und ^ durch Leerzeichen ersetzen\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    # 7) Mehrfache Leerzeichen reduzieren und trimmen\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# 5. Neue Spalte anlegen\n",
    "df['COMPONENT_cleaned'] = df['COMPONENT'].apply(clean_text)\n",
    "\n",
    "# 6. Speichern des neuen CSV\n",
    "df.to_csv(csv_out, index=False, encoding='utf-8')\n",
    "print(f\"Neues CSV erstellt: {csv_out} ({len(df)} Zeilen)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d872c73-037d-4909-b4fd-3ca80d38e629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neues CSV erstellt: /Users/mosimacnew/Code/speechbrain-fix/myNotebooks/2_STT-Metriken/mwer/make_korpus/Procedures_Drugs/deDE15LinguisticVariant_final.csv (18257 Zeilen)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Arbeitsverzeichnis ermitteln\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# 2. Eingangs- und Ausgangs-CSV\n",
    "csv_in  = os.path.join(base_dir, 'deDE15LinguisticVariant.csv')\n",
    "csv_out = os.path.join(base_dir, 'deDE15LinguisticVariant_final.csv')\n",
    "\n",
    "# 3. Nur die Spalte COMPONENT einlesen\n",
    "df = pd.read_csv(csv_in, usecols=['COMPONENT'], encoding='utf-8')\n",
    "\n",
    "# 4. Funktion zur Textbereinigung für COMPONENT_cleaned\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # Kleinbuchstaben\n",
    "    text = text.lower()\n",
    "    # Umlaute ersetzen\n",
    "    umlaut_map = {'ä':'ae','ö':'oe','ü':'ue','ß':'ss'}\n",
    "    for uml, rep in umlaut_map.items():\n",
    "        text = text.replace(uml, rep)\n",
    "    # Stop-Wörter entfernen\n",
    "    for w in ['oder','und','in']:\n",
    "        text = re.sub(r'\\b{}\\b'.format(w), ' ', text)\n",
    "    # \"XXX\" entfernen\n",
    "    text = text.replace('xxx', ' ')\n",
    "    # Bindestriche und Klammern durch Leerzeichen\n",
    "    text = text.replace('-', ' ')\n",
    "    text = text.replace('(', ' ').replace(')', ' ')\n",
    "    # Sonderzeichen, Interpunktion, Zahlen und ^ durch Leerzeichen ersetzen\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    # Einzelne Buchstaben entfernen (Wörter mit genau einem Zeichen)\n",
    "    text = re.sub(r'\\b[a-z]\\b', ' ', text)\n",
    "    # Mehrfache Leerzeichen reduzieren und trimmen\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# 5. Neue Spalte anlegen\n",
    "df['COMPONENT_cleaned'] = df['COMPONENT'].apply(clean_text)\n",
    "\n",
    "# 6. Speichern des neuen CSV\n",
    "df.to_csv(csv_out, index=False, encoding='utf-8')\n",
    "print(f\"Neues CSV erstellt: {csv_out} ({len(df)} Zeilen)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba4a82dd-8fcb-499c-9a14-ec2a38164ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeilen insgesamt vor Dedup: 18257\n",
      "Zeilen insgesamt nach Dedup: 6073\n",
      "Dedupliziertes CSV gespeichert: /Users/mosimacnew/Code/speechbrain-fix/myNotebooks/2_STT-Metriken/mwer/make_korpus/Procedures_Drugs/deDE15LinguisticVariant_final_nodup.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Arbeitsverzeichnis ermitteln\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# 2. Pfade definieren\n",
    "csv_in  = os.path.join(base_dir, 'deDE15LinguisticVariant_final.csv')\n",
    "csv_out = os.path.join(base_dir, 'deDE15LinguisticVariant_final_nodup.csv')\n",
    "\n",
    "# 3. CSV laden\n",
    "df = pd.read_csv(csv_in, usecols=['COMPONENT', 'COMPONENT_cleaned'], encoding='utf-8')\n",
    "\n",
    "# 4. Count vor dem Entfernen der Duplikate\n",
    "print(f\"Zeilen insgesamt vor Dedup: {len(df)}\")\n",
    "\n",
    "# 5. Duplikate anhand der bereinigten Spalte entfernen\n",
    "df_nodup = df.drop_duplicates(subset=['COMPONENT_cleaned'])\n",
    "\n",
    "# 6. Count nachher\n",
    "print(f\"Zeilen insgesamt nach Dedup: {len(df_nodup)}\")\n",
    "\n",
    "# 7. Ergebnis speichern\n",
    "df_nodup.to_csv(csv_out, index=False, encoding='utf-8')\n",
    "print(f\"Dedupliziertes CSV gespeichert: {csv_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc74123b-fbf6-4953-9ed6-340ee935df5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spalte 'ATC-Bedeutung_cleaned' wurde aktualisiert und '/Users/mosimacnew/Code/speechbrain-fix/myNotebooks/2_STT-Metriken/mwer/make_korpus/Procedures_Drugs/ATC-Bedeutung_final.csv' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Arbeitsverzeichnis ermitteln\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# 2. Pfad zur finalen CSV\n",
    "csv_path = os.path.join(base_dir, 'ATC-Bedeutung_final.csv')\n",
    "\n",
    "# 3. CSV einlesen (Standard-Komma-Delimiter)\n",
    "df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "\n",
    "# 4. Stopwörter definieren (einschließlich 'andere' und 'ohne')\n",
    "stopw = ['mit', 'von', 'und', 'bei', 'andere', 'ohne', 'anderen', 'exkl', 'nicht', 'haltige', 'zubereitung', 'kombinationen', 'haltige', 'verbindungen']\n",
    "\n",
    "def further_clean(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # alles in Kleinbuchstaben\n",
    "    text = text.lower()\n",
    "    # 1) Stopwörter entfernen\n",
    "    pattern_stop = r'\\b(?:' + '|'.join(stopw) + r')\\b'\n",
    "    text = re.sub(pattern_stop, ' ', text)\n",
    "    # 2) Einzelne Buchstaben entfernen\n",
    "    text = re.sub(r'\\b[a-z]\\b', ' ', text)\n",
    "    # 3) Mehrfache Leerzeichen auf ein Leerzeichen reduzieren und trimmen\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# 5. Spalte updaten\n",
    "df['ATC-Bedeutung_cleaned'] = df['ATC-Bedeutung_cleaned'].apply(further_clean)\n",
    "\n",
    "# 6. Änderung speichern\n",
    "df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "print(f\"Spalte 'ATC-Bedeutung_cleaned' wurde aktualisiert und '{csv_path}' gespeichert.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cafe9b63-4743-4ec3-87d8-a8e8143c77ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeilen vor Entfernen der Duplikate: 6026\n",
      "Zeilen nach Entfernen der Duplikate: 5477\n",
      "Bereinigtes Lexikon gespeichert als: /Users/mosimacnew/Code/speechbrain-fix/myNotebooks/2_STT-Metriken/mwer/make_korpus/Procedures_Drugs/lexikon_ATC-Bedeutung_final.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Arbeitsverzeichnis ermitteln\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# 2. Pfad zur aktuellen CSV\n",
    "csv_in  = os.path.join(base_dir, 'ATC-Bedeutung_final.csv')\n",
    "\n",
    "# 3. CSV einlesen\n",
    "df = pd.read_csv(csv_in, encoding='utf-8')\n",
    "\n",
    "# 4. Count vor Dedup\n",
    "print(f\"Zeilen vor Entfernen der Duplikate: {len(df)}\")\n",
    "\n",
    "# 5. Duplikate auf Basis der bereinigten Spalte entfernen\n",
    "df_nodup = df.drop_duplicates(subset=['ATC-Bedeutung_cleaned'])\n",
    "\n",
    "# 6. Count nachher\n",
    "print(f\"Zeilen nach Entfernen der Duplikate: {len(df_nodup)}\")\n",
    "\n",
    "# 7. Ausgabe-Dateiname mit Prefix\n",
    "csv_out = os.path.join(base_dir, 'lexikon_' + os.path.basename(csv_in))\n",
    "\n",
    "# 8. Speichern\n",
    "df_nodup.to_csv(csv_out, index=False, encoding='utf-8')\n",
    "print(f\"Bereinigtes Lexikon gespeichert als: {csv_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a7845-c315-4d0d-aab6-ebcc85fc29c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanic eval",
   "language": "python",
   "name": "stanic-eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
