{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e67f69-d929-480a-92cb-e71a765e5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ziel: Phrase-Level m-WER, bei dem jede WER-Operation als _med_ zählt, \n",
    "# wenn eine der enthaltenen Wörter medizinisch ist (Cutoff ≥ 0.80).\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from jiwer import process_words, collect_error_counts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Medizinisches Vokabular vorbereiten\n",
    "# -------------------------------\n",
    "cleaned = pd.read_csv(\"lexikon_cleaned_ger_synonyms.csv\")\n",
    "noun_set = set(w for ph in cleaned[\"only_nouns\"].dropna() for w in ph.split())\n",
    "adj_set  = set(w for ph in cleaned[\"adjectives\"].dropna()  for w in ph.split())\n",
    "\n",
    "atc   = pd.read_csv(\"lexikon_ATC-Bedeutung_final_noarticles.csv\")\n",
    "ling  = pd.read_csv(\"lexikon_deDE15LinguisticVariant_final_noarticles.csv\")\n",
    "\n",
    "def tokenize_list(phrases):\n",
    "    toks = set()\n",
    "    for p in phrases:\n",
    "        for w in re.sub(r\"[^\\w\\säöüß]\", \" \", str(p).lower()).split():\n",
    "            if w.isalpha(): toks.add(w)\n",
    "    return toks\n",
    "\n",
    "atc_set  = tokenize_list(atc[\"ATC-Bedeutung_cleaned\"].dropna())\n",
    "comp_set = tokenize_list(ling[\"COMPONENT_cleaned\"].dropna())\n",
    "\n",
    "med_vocab = sorted(noun_set.union(adj_set, atc_set, comp_set))\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2,4))\n",
    "X_vocab = vectorizer.fit_transform(med_vocab)\n",
    "\n",
    "# Batch-unique tokens from previous JSON (to build lookup)\n",
    "wer_data = json.load(open(\"wer_token_sources.json\", encoding=\"utf-8\"))\n",
    "unique_tokens = set()\n",
    "for e in wer_data:\n",
    "    unique_tokens.update(w for ph in e.get(\"subs_ref_tokens\", []) for w in ph.split())\n",
    "    unique_tokens.update(w for ph in e.get(\"del_ref_tokens\", [])   for w in ph.split())\n",
    "    unique_tokens.update(w for ph in e.get(\"ins_hyp_tokens\", [])   for w in ph.split())\n",
    "\n",
    "tokens_list = sorted(unique_tokens)\n",
    "X_tok = vectorizer.transform(tokens_list)\n",
    "sims_matrix = cosine_similarity(X_tok, X_vocab)\n",
    "token_to_score = {\n",
    "    tok: float(sims_matrix[i].max())\n",
    "    for i, tok in enumerate(tokens_list)\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Phrase-Level compute function\n",
    "# -------------------------------\n",
    "def compute_wer_mwer_phrase(ref, hyp, cutoff=0.80):\n",
    "    out       = process_words(ref, hyp)\n",
    "    subs_d, ins_d, del_d = collect_error_counts(out)\n",
    "    \n",
    "    # Standard counts\n",
    "    wer_score = out.wer\n",
    "    S = sum(subs_d.values())\n",
    "    D = sum(del_d.values())\n",
    "    I = sum(ins_d.values())\n",
    "\n",
    "    # Phrase-level medical counts\n",
    "    S_med = sum(\n",
    "        cnt for (ref_ph, _), cnt in subs_d.items()\n",
    "        if any(token_to_score.get(w,0.0) >= cutoff for w in ref_ph.split())\n",
    "    )\n",
    "    D_med = sum(\n",
    "        cnt for ref_ph, cnt in del_d.items()\n",
    "        if any(token_to_score.get(w,0.0) >= cutoff for w in ref_ph.split())\n",
    "    )\n",
    "    I_med = sum(\n",
    "        cnt for hyp_ph, cnt in ins_d.items()\n",
    "        if any(token_to_score.get(w,0.0) >= cutoff for w in hyp_ph.split())\n",
    "    )\n",
    "\n",
    "    # m-WER denominator: number of medical words in ref\n",
    "    ref_tokens = ref.split()\n",
    "    total_med_ref = sum(1 for w in ref_tokens if token_to_score.get(w,0.0) >= cutoff)\n",
    "    mwer = (S_med + D_med + I_med) / total_med_ref if total_med_ref else 0.0\n",
    "\n",
    "    return wer_score, S, D, I, S_med, D_med, I_med, mwer\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Über DB iterieren und CSV export\n",
    "# -------------------------------\n",
    "client = MongoClient(\"mongodb://localhost:27018/\")\n",
    "col = client[\"transcriptions\"][\"transcripts_denis\"]\n",
    "\n",
    "cursor = col.find(\n",
    "    {\"excludeGeneral\": 0},\n",
    "    {\"convoID\":1, \"ambientVariant\":1, \"processedVolume\":1,\n",
    "     \"technology\":1, \"model\":1,\n",
    "     \"src_wer_denis\":1, \"text_wer_denis\":1}\n",
    ")\n",
    "\n",
    "rows = []\n",
    "for doc in cursor:\n",
    "    ref = doc.get(\"src_wer_denis\",\"\")\n",
    "    hyp = doc.get(\"text_wer_denis\",\"\")\n",
    "    wer, S, D, I, S_med, D_med, I_med, mwer = compute_wer_mwer_phrase(ref, hyp)\n",
    "    rows.append({\n",
    "        \"convoID\":        doc.get(\"convoID\"),\n",
    "        \"ambientVariant\": doc.get(\"ambientVariant\"),\n",
    "        \"processedVolume\":doc.get(\"processedVolume\"),\n",
    "        \"technology\":     doc.get(\"technology\"),\n",
    "        \"model\":          doc.get(\"model\"),\n",
    "        \"wer\":            wer,\n",
    "        \"S\":              S,\n",
    "        \"D\":              D,\n",
    "        \"I\":              I,\n",
    "        \"S_med\":          S_med,\n",
    "        \"D_med\":          D_med,\n",
    "        \"I_med\":          I_med,\n",
    "        \"mwer\":           mwer\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"transcripts_wer_mwer_phrase.csv\", index=False)\n",
    "print(f\"{len(df)} Transcripts verarbeitet. Ergebnis in 'transcripts_wer_mwer_phrase.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanic eval",
   "language": "python",
   "name": "stanic-eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
