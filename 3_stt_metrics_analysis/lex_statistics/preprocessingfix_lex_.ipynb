{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b70d4e0-bb3d-4084-85a5-b6f4e393184f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup erstellt: 12000 Dokumente in 'transcripts_denis_prelexchange'\n"
     ]
    }
   ],
   "source": [
    "# 1) Backup der Collection\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Verbindung herstellen\n",
    "client = MongoClient(\"mongodb://localhost:27018/\")\n",
    "db     = client[\"transcriptions\"]\n",
    "src    = db[\"transcripts_denis\"]\n",
    "dst    = db[\"transcripts_denis_prelexchange\"]\n",
    "\n",
    "# Kopieren aller Dokumente\n",
    "dst.insert_many(src.find({}))\n",
    "print(f\"Backup erstellt: {dst.count_documents({})} Dokumente in 'transcripts_denis_prelexchange'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc6bca8-e648-49cd-9bc0-c80a02c9fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 Preprocessing Functions\n",
    "# ---------------------------------------------------------------\n",
    "import re\n",
    "import string\n",
    "from num2words import num2words\n",
    "from jiwer import (\n",
    "    Compose, Strip, RemoveWhiteSpace, RemovePunctuation,\n",
    "    ToLowerCase, RemoveMultipleSpaces, RemoveEmptyStrings,\n",
    "    SubstituteWords, ReduceToListOfListOfWords\n",
    ")\n",
    "# Für Lex Similarity (TD-IDF)\n",
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "SPACY_STOPWORDS = nlp.Defaults.stop_words\n",
    "\n",
    "# 9.1 jiwer–Pipeline für WER/CER/SER\n",
    "def get_wer_transforms():\n",
    "    return Compose([\n",
    "        Strip(),\n",
    "        RemoveWhiteSpace(replace_by_space=True),\n",
    "        SubstituteWords({\n",
    "            \"z. b.\": \"zum beispiel\",\n",
    "            \"dr.\":   \"doktor\"\n",
    "        }),\n",
    "        RemovePunctuation(),\n",
    "        ToLowerCase(),\n",
    "        RemoveMultipleSpaces(),\n",
    "        RemoveEmptyStrings(),\n",
    "        ReduceToListOfListOfWords()\n",
    "    ])\n",
    "\n",
    "# 9.2 Eigene Pipeline-Elemente für die anderen Metriken\n",
    "\n",
    "def norm_base(text: str) -> str:\n",
    "    \"\"\"Replace linebreaks, lowercase and normalize whitespace\"\"\"\n",
    "    t = text.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    return \" \".join(t.lower().split())\n",
    "\n",
    "# Abkürzungen inline behandelt\n",
    "def expand_abbrev_custom(text: str) -> str:\n",
    "    \"\"\"Expand common German abbreviations\"\"\"\n",
    "    mapping = {\n",
    "        r\"\\bz\\. b\\.?\\b\": \"zum beispiel\",\n",
    "        r\"\\bdr\\.?\\b\":   \"doktor\"\n",
    "    }\n",
    "    t = text\n",
    "    for pat, full in mapping.items():\n",
    "        t = re.sub(pat, full, t, flags=re.IGNORECASE)\n",
    "    return t\n",
    "\n",
    "# Umlaut-Normalisierung inline\n",
    "def normalize_umlaute(text: str) -> str:\n",
    "    \"\"\"Convert German umlauts and ß into ASCII equivalents\"\"\"\n",
    "    pattern = re.compile(r\"(?:ä|ö|ü|Ä|Ö|Ü|ß)\")\n",
    "    return pattern.sub(lambda m: {\n",
    "        'ä':'ae','ö':'oe','ü':'ue',\n",
    "        'Ä':'Ae','Ö':'Oe','Ü':'Ue','ß':'ss'\n",
    "    }[m.group(0)], text)\n",
    "\n",
    "# Punctuation removal\n",
    "def remove_punct(text: str) -> str:\n",
    "    \"\"\"Remove punctuation characters\"\"\"\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# Zahlenerweiterung und Verhältnis-Ersetzung\n",
    "\n",
    "def expand_slash_ratios(text: str) -> str:\n",
    "    \"\"\"Replace numerical ratios X/Y with 'X über Y'\"\"\"\n",
    "    return re.sub(r\"\\b(\\d+)\\s*/\\s*(\\d+)\\b\", r\"\\1 über \\2\", text)\n",
    "\n",
    "\n",
    "def expand_numbers(text: str, lang: str = 'de') -> str:\n",
    "    \"\"\"\n",
    "    Expand numeric tokens into words:\n",
    "      1) Dezimalzahlen mit Punkt: '97.9' → 'siebenundneunzig punkt neun'\n",
    "      2) Dezimalzahlen mit Komma: '97,9' → 'siebenundneunzig komma neun'\n",
    "      3) Ganze Zahlen: '65' → 'fünfundsechzig'\n",
    "    \"\"\"\n",
    "    # Verhältnisse zuerst\n",
    "    text = expand_slash_ratios(text)\n",
    "    # Dezimalzahlen\n",
    "    def repl_decimal(m):\n",
    "        whole, sep, frac = m.group(1), m.group(2), m.group(3)\n",
    "        w = num2words(int(whole), lang=lang)\n",
    "        f = num2words(int(frac), lang=lang)\n",
    "        sep_word = \"punkt\" if sep == \".\" else \"komma\"\n",
    "        return f\"{w} {sep_word} {f}\"\n",
    "    text = re.sub(r\"\\b(\\d+)([.,])(\\d+)\\b\", repl_decimal, text)\n",
    "    # Ganze Zahlen\n",
    "    text = re.sub(\n",
    "        r\"\\b(\\d+)\\b\",\n",
    "        lambda m: num2words(int(m.group(1)), lang=lang),\n",
    "        text\n",
    "    )\n",
    "    return text\n",
    "\n",
    "# 9.3 Pipelines for specific metrics\n",
    "\n",
    "def preprocess_text_for_wer(text: str) -> str:\n",
    "    t = norm_base(text)\n",
    "    t = expand_abbrev_custom(t)\n",
    "    t = normalize_umlaute(t)\n",
    "    t = expand_numbers(t)\n",
    "    t = remove_punct(t)\n",
    "    return t\n",
    "\n",
    "\n",
    "def preprocess_text_for_lexical_cosine(text: str) -> str:\n",
    "    t = norm_base(text)\n",
    "    t = expand_abbrev_custom(t)\n",
    "    t = normalize_umlaute(t)\n",
    "    t = expand_numbers(t)\n",
    "    t = remove_punct(t)\n",
    "    tokens = [tok for tok in t.split() if tok not in SPACY_STOPWORDS]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def preprocess_text_for_semantic_cosine(text: str) -> str:\n",
    "    \"\"\"Minimal preprocessing: normalize whitespace & lowercase\"\"\"\n",
    "    return norm_base(text)\n",
    "\n",
    "# 9.4 Pipelines for NER (MEER)\n",
    "import spacy\n",
    "\n",
    "# Lade spaCy-Modell einmalig\n",
    "_nlp = spacy.load(\"de_core_news_lg\")\n",
    "\n",
    "# Truecasing-Funktion für Modelle ohne natives Casing\n",
    "# Truecasing-Funktion für Modelle ohne natives Casing\n",
    "def truecase_text(text: str) -> str:\n",
    "    \"\"\"Lowercase-Then-Truecase basierend auf POS: Satzanfang & Substantive/Eigenname\"\"\"\n",
    "    text = text.lower()\n",
    "    doc = _nlp(text)\n",
    "    result = []\n",
    "    capitalize_next = True\n",
    "    for token in doc:\n",
    "        # Satzanfang oder Substantiv/Eigenname groß\n",
    "        if capitalize_next or token.pos_ in {\"NOUN\", \"PROPN\"}:\n",
    "            result.append(token.text.capitalize())\n",
    "        else:\n",
    "            result.append(token.text)\n",
    "        capitalize_next = token.text in {\".\", \"!\", \"?\"}\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "# Preprocessing für NER: nur text_meer_denis, unter Berücksichtigung des Modells\n",
    "def preprocess_text_for_ner_meer(text: str, model: str) -> str:\n",
    "    \"\"\"\n",
    "    Minimal preprocessing for BERT-based NER models (MEER).\n",
    "    - Strip + normalize whitespace\n",
    "    - Truecase für model-specific cases\n",
    "    \"\"\"\n",
    "    # Grundlegendes Strip/Whitespace\n",
    "    cleaned = \" \".join(text.strip().split())\n",
    "    # Truecasing nur für spez. ASR-Modelle\n",
    "    if model in {\"vosk-model-de-0.21\", \"whisper_rescuespeech\"}:\n",
    "        return truecase_text(cleaned)\n",
    "    return cleaned\n",
    "\n",
    "# Usage:\n",
    "#  ner_input = preprocess_text_for_ner_meer(doc_text, doc_model)\n",
    "#  src_meer_denis bleibt unverändert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "416a3109-3f09-4336-9526-c3ce32d3cfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprocessing abgeschlossen: 'text_lex_denis' und 'src_lex_denis' neu befüllt.\n"
     ]
    }
   ],
   "source": [
    "# 2) Alte Felder löschen und lex-Pipeline neu anwenden\n",
    "from pymongo import MongoClient\n",
    "import spacy\n",
    "\n",
    "\n",
    "\n",
    "# Verbindung und Collection\n",
    "client = MongoClient(\"mongodb://localhost:27018/\")\n",
    "db     = client[\"transcriptions\"]\n",
    "coll   = db[\"transcripts_denis\"]\n",
    "\n",
    "# Update aller Dokumente\n",
    "for doc in coll.find({}):\n",
    "    raw = doc.get(\"text\", \"\")\n",
    "    ref = doc.get(\"srcText\", \"\")\n",
    "    new_text_lex = preprocess_text_for_lexical_cosine(raw)\n",
    "    new_src_lex  = preprocess_text_for_lexical_cosine(ref)\n",
    "\n",
    "    coll.update_one(\n",
    "        {\"_id\": doc[\"_id\"]},\n",
    "        {\"$unset\": {\"text_lex_denis\": \"\", \"src_lex_denis\": \"\"}}\n",
    "    )\n",
    "    coll.update_one(\n",
    "        {\"_id\": doc[\"_id\"]},\n",
    "        {\"$set\": {\"text_lex_denis\": new_text_lex, \"src_lex_denis\": new_src_lex}}\n",
    "    )\n",
    "\n",
    "print(\"Reprocessing abgeschlossen: 'text_lex_denis' und 'src_lex_denis' neu befüllt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7402c-a903-4ed5-ab33-5c1758934917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanic eval",
   "language": "python",
   "name": "stanic-eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
