{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0700a7-8111-4608-91e7-19f5a8e022ae",
   "metadata": {},
   "source": [
    "# Functions for Preprosessing \n",
    "## for Metric Calculations with Data out of MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89260b2e-f6e3-40f1-9998-459bbec8f573",
   "metadata": {},
   "source": [
    "## Semantic Cosine Preprocessing-Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11ebf338-e426-48b9-babd-5d60e61d29e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Semantic Preprocessing: 120it [00:00, 376.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 Dokumente mit Semantic-Preprocessing aktualisiert.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 11) Semantic-Cosine–Preprocessing ausführen und in MongoDB ersetzen\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) Verbindung & Collection\n",
    "client = MongoClient(\"mongodb://localhost:27018/\")\n",
    "db     = client[\"transcriptions\"]\n",
    "coll   = db[\"transcripts_denis\"]\n",
    "\n",
    "# 2) Loop über alle Docs und Felder neu setzen\n",
    "count = 0\n",
    "for doc in tqdm(coll.find({\"excludeGeneral\": 1}), desc=\"Semantic Preprocessing\"):\n",
    "    raw_ref = doc.get(\"srcText\", \"\")\n",
    "    raw_hyp = doc.get(\"text\", \"\")\n",
    "\n",
    "    # Nutze die finale, korrekte Funktion\n",
    "    sem_ref = preprocess_text_for_semantic_cosine(raw_ref)\n",
    "    sem_hyp = preprocess_text_for_semantic_cosine(raw_hyp)\n",
    "\n",
    "    coll.update_one(\n",
    "        {\"_id\": doc[\"_id\"]},\n",
    "        {\"$set\": {\n",
    "            \"src_sem_denis\": sem_ref,\n",
    "            \"text_sem_denis\": sem_hyp\n",
    "        }}\n",
    "    )\n",
    "    count += 1\n",
    "\n",
    "print(f\"{count} Dokumente mit Semantic-Preprocessing aktualisiert.\")\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6467cef5-79d1-4b76-a57c-12e57f66688e",
   "metadata": {},
   "source": [
    "# Check if key is in env variable\n",
    "import os\n",
    "print(os.getenv(\"OPEN_API_KEY_STANIC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d0d176-5a72-4850-b2a9-21126c85f68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding-Dimension: 3072\n",
      "Cosine Similarity:   0.8885\n"
     ]
    }
   ],
   "source": [
    "# 1) Imports & Client‐Setup CHECK\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = os.getenv(\"OPEN_API_KEY_STANIC\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Bitte setze die Umgebungsvariable OPEN_API_KEY_STANIC.\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# 2) Helper für Embeddings\n",
    "def get_embedding(text: str, model: str=\"text-embedding-3-large\") -> np.ndarray:\n",
    "    resp = client.embeddings.create(model=model, input=[text])\n",
    "    # im neuen Schema ist der Pfad: resp.data[0].embedding\n",
    "    return np.array(resp.data[0].embedding)\n",
    "\n",
    "# 3) Zwei kurze Beispiele\n",
    "src_text = \"Wir sind in der Wohnung. Paramedic Schmidt erklärt die Situation.\"\n",
    "hyp_text = \"wir sind in der wohnung paramedic schmidt erklaert situation\"\n",
    "\n",
    "# 4) Embeddings abrufen\n",
    "emb_src = get_embedding(src_text)\n",
    "emb_hyp = get_embedding(hyp_text)\n",
    "\n",
    "# 5) Cosine Similarity berechnen\n",
    "sim = float(cosine_similarity([emb_src], [emb_hyp])[0][0])\n",
    "\n",
    "# 6) Ausgabe\n",
    "print(f\"Embedding-Dimension: {len(emb_src)}\")   # sollte 3072 sein wenn text-embedding-3-large default dim\n",
    "print(f\"Cosine Similarity:   {sim:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0ba06e7-1908-4ec1-a776-37012dc6a5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|█████████████████████████████▉| 297/298 [13:28<00:02,  2.72s/it]\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json  # neu\n",
    "from datetime import datetime\n",
    "\n",
    "# 1) OpenAI‐Client\n",
    "#api_key   = os.getenv(\"OPEN_API_KEY_STANIC\")\n",
    "client_oa = OpenAI(api_key=api_key)\n",
    "\n",
    "def batch_embeddings(texts: list[str], model: str=\"text-embedding-3-large\") -> list[np.ndarray]:\n",
    "    resp = client_oa.embeddings.create(model=model, input=texts)\n",
    "    return [np.array(d.embedding) for d in resp.data]\n",
    "\n",
    "# 2) Mongo öffnen und bereits vorprocessed Docs holen\n",
    "mongo      = MongoClient(\"mongodb://localhost:27018/\")\n",
    "coll       = mongo[\"transcriptions\"][\"transcripts_denis\"]\n",
    "docs       = list(coll.find({\n",
    "    \"excludeGeneral\": 0,\n",
    "    \"src_sem_denis\":  {\"$exists\": True},\n",
    "    \"text_sem_denis\": {\"$exists\": True}\n",
    "}))\n",
    "\n",
    "# 2a) JSONL-File öffnen (neu)\n",
    "timestamp = datetime.now().strftime(\"%d%m%y_%H%M\")\n",
    "out_dir   = f\"sem_embeddings_{timestamp}\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "jsonl_f   = open(f\"{out_dir}/embeddings.jsonl\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "# 3) Batch-Größe\n",
    "BATCH_SIZE = 40\n",
    "\n",
    "# 4) In Batches durchlaufen\n",
    "for i in tqdm(range(0, len(docs), BATCH_SIZE), \n",
    "              total=len(docs)//BATCH_SIZE+1, desc=\"Batches\"):\n",
    "    batch     = docs[i : i + BATCH_SIZE]\n",
    "    src_texts = [d[\"src_sem_denis\"]  for d in batch]\n",
    "    hyp_texts = [d[\"text_sem_denis\"] for d in batch]\n",
    "\n",
    "    emb_srcs = batch_embeddings(src_texts)\n",
    "    emb_hyps = batch_embeddings(hyp_texts)\n",
    "\n",
    "    for doc, e_src, e_hyp in zip(batch, emb_srcs, emb_hyps):\n",
    "        sem_cos = float(cosine_similarity([e_src], [e_hyp])[0][0])\n",
    "\n",
    "        # 4a) Mongo-Update\n",
    "        coll.update_one(\n",
    "            {\"_id\": doc[\"_id\"]},\n",
    "            {\"$set\": {\"sem_cos_denis\": sem_cos}}\n",
    "        )\n",
    "\n",
    "        # 4b) JSONL schreiben (neu)\n",
    "        jsonl_f.write(json.dumps({\n",
    "            \"_id\":           str(doc[\"_id\"]),\n",
    "            \"technology\":    doc.get(\"technology\"),\n",
    "            \"model\":         doc.get(\"model\"),\n",
    "            \"fileName\":      doc.get(\"fileName\"),\n",
    "            \"convoID\":       doc.get(\"convoID\"),\n",
    "            \"ambientVariant\":doc.get(\"ambientVariant\"),\n",
    "            \"processedVolume\":doc.get(\"processedVolume\"),\n",
    "            \"sem_cos_denis\": sem_cos,\n",
    "            \"src_embeddings\": e_src.tolist(),\n",
    "            \"text_embeddings\":e_hyp.tolist()\n",
    "        }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# 5) Aufräumen\n",
    "jsonl_f.close()\n",
    "mongo.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cfce02-a40e-426d-aa8f-954faeea79f2",
   "metadata": {},
   "source": [
    "## Check if the the embedding and cos_sim worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82d726c9-9214-4da9-8f9a-f81d119aedc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Überprüfe Datei: sem_embeddings_040725_1817/embeddings.jsonl\n",
      "\n",
      "Gesamtanzahl Zeilen: 11880\n",
      "✅ Alle Einträge enthalten alle benötigten Keys.\n",
      "✅ Alle Embeddings haben die korrekte Dimension (1536).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "# 1) Finde das neueste JSONL-File\n",
    "jsonl_files = sorted(glob(\"sem_embeddings_*/embeddings.jsonl\"))\n",
    "if not jsonl_files:\n",
    "    raise FileNotFoundError(\"Keine embeddings.jsonl Datei gefunden.\")\n",
    "jsonl_path = jsonl_files[-1]\n",
    "print(f\"Überprüfe Datei: {jsonl_path}\\n\")\n",
    "\n",
    "# 2) Initialisiere Zähler und Sammler\n",
    "total_lines = 0\n",
    "missing_entries = []\n",
    "dim_mismatches = []\n",
    "\n",
    "# 3) Durchlaufe jede Zeile und prüfe Felder und Dimensions\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for idx, line in enumerate(f, start=1):\n",
    "        rec = json.loads(line)\n",
    "        total_lines += 1\n",
    "        \n",
    "        # Prüfe auf notwendige Keys\n",
    "        for key in [\"_id\", \"technology\", \"model\", \"fileName\", \"convoID\", \n",
    "                    \"ambientVariant\", \"processedVolume\", \"sem_cos_denis\", \n",
    "                    \"src_embeddings\", \"text_embeddings\"]:\n",
    "            if key not in rec:\n",
    "                missing_entries.append((idx, key))\n",
    "        \n",
    "        # Prüfe Embedding-Dimensionen\n",
    "        if \"src_embeddings\" in rec and len(rec[\"src_embeddings\"]) != 3072:\n",
    "            dim_mismatches.append((\"src\", idx, len(rec[\"src_embeddings\"])))\n",
    "        if \"text_embeddings\" in rec and len(rec[\"text_embeddings\"]) != 3072:\n",
    "            dim_mismatches.append((\"text\", idx, len(rec[\"text_embeddings\"])))\n",
    "\n",
    "# 4) Ergebnisse ausgeben\n",
    "print(f\"Gesamtanzahl Zeilen: {total_lines}\")\n",
    "if not missing_entries:\n",
    "    print(\"✅ Alle Einträge enthalten alle benötigten Keys.\")\n",
    "else:\n",
    "    print(\"❌ Fehlende Keys in folgenden Zeilen:\")\n",
    "    for line, key in missing_entries:\n",
    "        print(f\"  Zeile {line}: fehlt Key '{key}'\")\n",
    "\n",
    "if not dim_mismatches:\n",
    "    print(\"✅ Alle Embeddings haben die korrekte Dimension (3072).\")\n",
    "else:\n",
    "    print(\"❌ Dimensionsabweichungen gefunden:\")\n",
    "    for which, line, dim in dim_mismatches:\n",
    "        print(f\"  Zeile {line}: {which}_embeddings Länge = {dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1209e302-2463-4181-b65c-759734a1a9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Zeilen insgesamt: 11880\n",
      "❌ Fehlende/NaN‐Einträge in Zeilen: []\n",
      "✅ Vollständige Einträge: 11880\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.958542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median</th>\n",
       "      <td>0.974911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StdDev</th>\n",
       "      <td>0.041282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min</th>\n",
       "      <td>0.646001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max</th>\n",
       "      <td>0.998488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Value\n",
       "Mean    0.958542\n",
       "Median  0.974911\n",
       "StdDev  0.041282\n",
       "Min     0.646001\n",
       "Max     0.998488"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pfad zur JSONL-Datei – ggf. anpassen\n",
    "jsonl_path = \"sem_embeddings_040725_1817/embeddings.jsonl\"\n",
    "\n",
    "# 1) Einträge prüfen auf fehlende bzw. NaN‐Werte\n",
    "missing = []\n",
    "sem_cos = []\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        rec = json.loads(line)\n",
    "        val = rec.get(\"sem_cos_denis\")\n",
    "        if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "            missing.append(i)\n",
    "        else:\n",
    "            sem_cos.append(val)\n",
    "\n",
    "print(f\"📄 Zeilen insgesamt: {i}\")\n",
    "print(f\"❌ Fehlende/NaN‐Einträge in Zeilen: {missing[:10]}{'...' if len(missing)>10 else ''}\")\n",
    "print(f\"✅ Vollständige Einträge: {len(sem_cos)}\")\n",
    "\n",
    "# 2) Deskriptive Statistik\n",
    "stats = {\n",
    "    \"Mean\":    np.mean(sem_cos),\n",
    "    \"Median\":  np.median(sem_cos),\n",
    "    \"StdDev\":  np.std(sem_cos, ddof=1),\n",
    "    \"Min\":     np.min(sem_cos),\n",
    "    \"Max\":     np.max(sem_cos)\n",
    "}\n",
    "\n",
    "# 3) Ausgabe als DataFrame\n",
    "df = pd.DataFrame.from_dict(stats, orient=\"index\", columns=[\"Value\"])\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7119043b-8cd6-48ae-b595-75213eaa375d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV geschrieben: sem_cos_results.csv\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# 1) Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 2) Pfad zur JSONL-Datei – passe das Datums-/Zeit-Stempel-Verzeichnis an, falls nötig\n",
    "jsonl_dir  = sorted([d for d in os.listdir() if d.startswith(\"sem_embeddings_\")])[-1]\n",
    "jsonl_path = os.path.join(jsonl_dir, \"embeddings.jsonl\")\n",
    "\n",
    "# 3) JSONL als DataFrame laden\n",
    "df = pd.read_json(jsonl_path, lines=True)\n",
    "\n",
    "# 4) Nur die gewünschten Spalten auswählen\n",
    "df_csv = df[[\n",
    "    \"convoID\",\n",
    "    \"ambientVariant\",\n",
    "    \"processedVolume\",\n",
    "    \"technology\",\n",
    "    \"model\",\n",
    "    \"fileName\",\n",
    "    \"sem_cos_denis\"\n",
    "]]\n",
    "\n",
    "# 5) CSV speichern im Notebook-Ordner\n",
    "out_csv = \"sem_cos_results.csv\"\n",
    "df_csv.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ CSV geschrieben: {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "763780ec-51f5-493a-b72c-3d2465cd9765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Imports & MongoDB-Verbindung\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Verbinden und Collection auswählen\n",
    "client = MongoClient(\"mongodb://localhost:27018/\")\n",
    "db     = client[\"transcriptions\"]\n",
    "coll   = db[\"transcripts_denis\"]\n",
    "\n",
    "# Alle Dokumente (ohne manuell ausgesperrte)\n",
    "docs = list(coll.find({\"excludeGeneral\": 0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17706910-ce2b-470b-b5f6-86046d12da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) TF-IDF auf gesamten Korpus fitten\n",
    "# Wir brauchen globale IDF-Werte, also fitten wir auf ALLE Vorverarbeitungen:\n",
    "lex_hyps = [doc[\"text_lex_denis\"] for doc in docs]\n",
    "lex_refs = [doc[\"src_lex_denis\"] for doc in docs]\n",
    "corpus   = lex_hyps + lex_refs\n",
    "\n",
    "vectorizer    = TfidfVectorizer()\n",
    "tfidf_matrix  = vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a4b926e-c2f6-4ee9-ac34-487f39856af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Cosine Similarity scores saved to lexical_cosine_scores_full.csv\n"
     ]
    }
   ],
   "source": [
    "# 3) Paarweise Cosine-Ähnlichkeit berechnen und in DataFrame packen\n",
    "rows = []\n",
    "N = len(docs)\n",
    "for i, doc in enumerate(docs):\n",
    "    hyp_vec = tfidf_matrix[i]\n",
    "    ref_vec = tfidf_matrix[i + N]\n",
    "    sim     = cosine_similarity(hyp_vec, ref_vec)[0][0]\n",
    "\n",
    "    rows.append({\n",
    "        \"convoID\":           doc.get(\"convoID\"),\n",
    "        \"ambientVariant\":    doc.get(\"ambientVariant\"),\n",
    "        \"processedVolume\":   doc.get(\"processedVolume\"),\n",
    "        \"technology\":        doc.get(\"technology\"),\n",
    "        \"model\":             doc.get(\"model\"),\n",
    "        \"lex_cosine_sim\":    sim\n",
    "    })\n",
    "\n",
    "df_lex = pd.DataFrame(rows)\n",
    "df_lex.to_csv(\"lexical_cosine_scores_full.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Lexical Cosine Similarity scores saved to lexical_cosine_scores_full.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanic eval",
   "language": "python",
   "name": "stanic-eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
